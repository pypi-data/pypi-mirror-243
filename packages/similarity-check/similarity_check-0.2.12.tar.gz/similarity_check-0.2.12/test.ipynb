{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "os.environ['s3_endpoint'] = 'https://jed-s3.bluvalt.com/'\n",
    "os.environ['s3_bucket_name'] = 'psj1-dtm-s3-mr01'\n",
    "os.environ['s3_access_key'] = 'UW0888TKFM7IJELCM4C1'\n",
    "os.environ['s3_secret_key'] = 't9D++D0o5mw+03j0DgcXJViQftYVV/29o/KmtmL4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'boto3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/m.aldossari/LEAN/Similarity Check Package/similarity_check/test.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.20.196.121/home/m.aldossari/LEAN/Similarity%20Check%20Package/similarity_check/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msimilarity_check\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m upload_object, download_object\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.20.196.121/home/m.aldossari/LEAN/Similarity%20Check%20Package/similarity_check/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Union, List, Optional, Dict\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.20.196.121/home/m.aldossari/LEAN/Similarity%20Check%20Package/similarity_check/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n",
      "File \u001b[0;32m~/LEAN/Similarity Check Package/similarity_check/src/similarity_check/utils.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mboto3\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpickle\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'boto3'"
     ]
    }
   ],
   "source": [
    "from src.similarity_check.utils import upload_object, download_object\n",
    "from typing import Union, List, Optional, Dict\n",
    "import os\n",
    "import json\n",
    "from nltk.stem import PorterStemmer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "from sentence_transformers import SentenceTransformer, util, InputExample, losses, models\n",
    "from string import punctuation\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from huggingface_hub.utils._errors import HTTPError\n",
    "import re\n",
    "import pickle\n",
    "download('stopwords')\n",
    "        \n",
    "\n",
    "def preprocess(\n",
    "    sentence: str, \n",
    "    replace_file_path: str=None,\n",
    "    remove_punct: bool=False, \n",
    "    separate_punct: bool=False,\n",
    "    remove_stop_words: bool=False, \n",
    "    stemm: bool=False, \n",
    "    separate_numbers: bool=False,\n",
    "    lang: str='en'\n",
    ") -> str: \n",
    "    sentence = str(sentence) # ensure all sentences are string, even if they are some texts with number only\n",
    "    try:\n",
    "        if isinstance(replace_file_path, str):\n",
    "            df = pd.read_excel(replace_file_path)\n",
    "            replace_dict = dict(zip(df['from'], df['to']))\n",
    "        else:\n",
    "            replace_dict = {}\n",
    "    except NameError:\n",
    "        print(f'provided replace file doesn\\'t have required columns, available columns are : {df.columns}')\n",
    "        replace_dict = {}\n",
    "    except FileNotFoundError:\n",
    "        print(f'provided replace file doesn\\'t exists')\n",
    "        replace_dict = {}\n",
    "\n",
    "    ps = PorterStemmer()\n",
    "    # separate numbers from characters\n",
    "    if separate_numbers:\n",
    "        sentence = re.sub(r' ?(\\d+) ?', r' \\1 ', sentence)\n",
    "\n",
    "    if remove_punct: # remove punctuations\n",
    "        sentence = sentence.translate(str.maketrans('', '', punctuation))\n",
    "    elif separate_punct: # separate punctuations\n",
    "        for punct in punctuation:\n",
    "            sentence = sentence.replace(punct, f' {punct} ')\n",
    "    if lang.lower() == 'en':\n",
    "        ps = PorterStemmer()\n",
    "        # remove stop words and stem\n",
    "        if remove_stop_words and stemm:\n",
    "            stop_words = stopwords.words('english')\n",
    "            return ' '.join([ps.stem(replace_dict.get(w, w)) for w in sentence.lower().split() if w not in stop_words])\n",
    "        # stem only\n",
    "        elif not remove_stop_words and stemm:\n",
    "            return ' '.join([ps.stem(replace_dict.get(w, w)) for w in sentence.lower().split()])\n",
    "        else:\n",
    "            # lower case and remove extra white spaces\n",
    "            return ' '.join([replace_dict.get(w, w) for w in sentence.lower().split()])\n",
    "    elif lang.lower() == 'ar':\n",
    "        st = ISRIStemmer()\n",
    "        # remove stop words and stem\n",
    "        if remove_stop_words and stemm:\n",
    "            download('stopwords')\n",
    "            stop_words = stopwords.words('arabic')\n",
    "            return ' '.join([st.stem(w) for w in sentence.lower().split() if w not in stop_words])\n",
    "        # stem only\n",
    "        elif not remove_stop_words and stemm:\n",
    "            return ' '.join([st.stem(w) for w in sentence.lower().split()])\n",
    "        else:\n",
    "            # lower case and remove extra white spaces\n",
    "            return ' '.join([w for w in sentence.lower().split()])\n",
    "    else:\n",
    "        raise Exception('non recognized language please specify either en|ar')\n",
    "\n",
    "    \n",
    "class sentence_tranformer_checker():\n",
    "    def __init__(\n",
    "        self, \n",
    "        device: Optional[str] = None,\n",
    "        model: Optional[str]=None, \n",
    "        lang: Optional[str]='en', \n",
    "        encode_batch: Optional[int] = 32,\n",
    "        show_prograss: Optional[bool] = False,\n",
    "        separate_numbers: Optional[bool]=True,\n",
    "        separate_punct: Optional[bool]=True,\n",
    "        remove_punct: Optional[bool]=False, \n",
    "        remove_stop_words: Optional[bool]=False, \n",
    "        stemm: Optional[bool]=False,\n",
    "        replace_file_path: str=None\n",
    "    ):    \n",
    "        \"\"\"\n",
    "        parameters:\n",
    "            device: the device to do the encoding on operations in (cpu|cuda),\n",
    "            model (optional): a string of the sentence tranformer model, to use instead of the default one, for more [details](https://www.sbert.net/).\n",
    "            lang (optional): the languge of the model ('en'|'ar').\n",
    "            only_include (optional): used only for dataframe matching, allow providing a list of column names to only include for the target matches, provide empty list to get only target_col.\n",
    "            encode_batch (optional): the number of sentences to encode in a batch.\n",
    "            encode_target: boolean flag to indicate whatever to enocde the targets when initilizing the object (to cache target encoding).\n",
    "            remove_punct: boolean flag to indicate whatever to remove punctuations. \n",
    "            remove_stop_words: boolean flag to indicate whatever to remove stop words.\n",
    "            stemm: boolean flag to indicate whatever to do stemming.\n",
    "        \"\"\"\n",
    "\n",
    "        self.encode_batch = encode_batch\n",
    "        self.remove_punct = remove_punct\n",
    "        self.remove_stop_words = remove_stop_words\n",
    "        self.stemm = stemm\n",
    "        self.lang = lang \n",
    "        self.show_prograss = show_prograss\n",
    "        self.separate_numbers = separate_numbers\n",
    "        self.separate_punct = separate_punct\n",
    "        self.replace_file_path = replace_file_path\n",
    "            \n",
    "        if device is None:\n",
    "            self.device = None\n",
    "        else:\n",
    "            self.device = device.lower()\n",
    "\n",
    "        # for target in self.targets.values():\n",
    "        #     if pd.isnull(target).any():\n",
    "        #         raise ValueError('Targets contain null values')\n",
    "\n",
    "        if model is not None:\n",
    "            try:\n",
    "                self.model = SentenceTransformer(model, device=self.device)\n",
    "                if self.show_prograss:\n",
    "                    print('done...')\n",
    "            except HTTPError:\n",
    "                raise HTTPError('entered model name is not defined')\n",
    "        # if no model is provided use the default model\n",
    "        else:\n",
    "            if self.show_prograss:\n",
    "                print('initializing the model...')\n",
    "            if lang.lower() == 'en':\n",
    "                self.model = SentenceTransformer('all-MiniLM-L6-v2', device=self.device)\n",
    "                if self.show_prograss:\n",
    "                    print('done...')\n",
    "            else:\n",
    "                self.model = SentenceTransformer('distiluse-base-multilingual-cased-v1', device=self.device)\n",
    "                if self.show_prograss:\n",
    "                    print('done...')\n",
    "\n",
    "\n",
    "    # used to make sure that if the object targets are updated, the cached targets embeddings are deleted\n",
    "    def __setattr__(self, key, value):\n",
    "        # self.key = value\n",
    "        if key == 'targets' or key == 'target_df':\n",
    "            if hasattr(self, 'encoded_targets_dict'):\n",
    "                del self.encoded_targets_dict     \n",
    "        super().__setattr__(key, value)\n",
    "\n",
    "\n",
    "    def init_targets(\n",
    "        self,\n",
    "        targets: Union[List[str], pd.DataFrame], \n",
    "        target_group: Optional[Union[List[str], List[int]]]=None,\n",
    "        target_cols: Optional[List[str]]=None, \n",
    "        only_include: Optional[List[str]]=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        targets: dataframe or list of targets text to compare with.\n",
    "        target_group (optional): goups ids for the target to match only a single target for each group, can either provide list of ids,\n",
    "        or the column name in the target dataframe.\n",
    "        target_cols (partially optional): the target column names used to match, *must be specified for dataframe matching*.\n",
    "        \"\"\"\n",
    "        if isinstance(target_cols, str):\n",
    "            target_cols = [target_cols]\n",
    "\n",
    "        if isinstance(targets, pd.DataFrame):\n",
    "            targets = targets.copy(deep=True)\n",
    "\n",
    "            for target_col in target_cols:\n",
    "                if target_col not in targets.columns:\n",
    "                    raise KeyError('target_col not found in target DataFrame cloumns')\n",
    "                else:\n",
    "                    targets.loc[:, target_col] = targets[target_col].fillna('')\n",
    "\n",
    "            if target_group is not None:\n",
    "                if isinstance(target_group, str):\n",
    "                    if target_group not in targets.columns:\n",
    "                        raise KeyError('target_group not found in target DataFrame cloumns')\n",
    "                    self.group_ids = targets[target_group].tolist()\n",
    "                else:\n",
    "                    self.group_ids = target_group\n",
    "            else:\n",
    "                self.group_ids = None\n",
    "                \n",
    "            if only_include is not None:\n",
    "                for col_name in only_include:\n",
    "                    if col_name not in targets.columns:       \n",
    "                        raise KeyError(f'only_include value:({col_name}) not found not found in target DataFrame cloumns')    \n",
    "                for target_col in target_cols:\n",
    "                    only_include.insert(0, target_col)\n",
    "                \n",
    "                targets = targets.loc[:, only_include]\n",
    "                \n",
    "\n",
    "            self.target_df = targets.reset_index(drop=True)\n",
    "            self.targets = {target_col: [\n",
    "                        preprocess(\n",
    "                        sent, \n",
    "                        self.replace_file_path,\n",
    "                        self.remove_punct, \n",
    "                        self.separate_punct,\n",
    "                        self.remove_stop_words, \n",
    "                        self.stemm, \n",
    "                        self.separate_numbers,\n",
    "                        self.lang)  \n",
    "                        for sent in targets[target_col].tolist()\n",
    "                    ] \n",
    "                for target_col in target_cols}\n",
    "        elif isinstance(targets, list):\n",
    "            if isinstance(target_group, list):\n",
    "                self.group_ids = target_group\n",
    "            else:\n",
    "                if target_group is None:\n",
    "                    self.group_ids = target_group\n",
    "                else:\n",
    "                    raise TypeError('if target are a list, provided groups must also be a list')\n",
    "        \n",
    "            if not targets:\n",
    "                raise TypeError('Targets are empty') \n",
    "\n",
    "            self.target_df = pd.DataFrame({'target': targets})\n",
    "            self.targets = {'target': [\n",
    "                    preprocess(\n",
    "                    sent, \n",
    "                    self.replace_file_path,\n",
    "                    self.remove_punct, \n",
    "                    self.separate_punct,\n",
    "                    self.remove_stop_words, \n",
    "                    self.stemm, \n",
    "                    self.separate_numbers,\n",
    "                    self.lang)  \n",
    "                for sent in targets]\n",
    "            }\n",
    "        else:\n",
    "            msg = f'targets must be a dataframe or a list, instead a source of type {str(type(targets))} was passed'\n",
    "            raise TypeError(msg)\n",
    "          \n",
    "        if self.show_prograss:\n",
    "            print('enocding targets: ')\n",
    "            \n",
    "        self.encoded_targets_dict = {(k): (self.model.encode(v, batch_size=self.encode_batch, show_progress_bar=self.show_prograss,  normalize_embeddings=True)) for k, v in self.targets.items()} # encode the targets\n",
    "        \n",
    "\n",
    "    def save_targets(self, targets_save_path, save_to='file_system'):\n",
    "        if not hasattr(self, 'encoded_targets_dict'):\n",
    "            raise ValueError('The targets was not initialized, please use one of the following functions to initialize the encoded_targets_dict: (init_targets, load_targets)')\n",
    "        if save_to == 'file_system':\n",
    "            if not os.path.exists(targets_save_path):\n",
    "                os.makedirs(targets_save_path)\n",
    "\n",
    "            # for encoded_target_name, encoded_target in self.encoded_targets_dict.items():  \n",
    "                \n",
    "                \n",
    "            #     if not os.path.exists(encoded_target_dir):\n",
    "            #         os.mkdir(encoded_target_dir)\n",
    "                    \n",
    "            #     np.save(os.path.join(encoded_target_dir, f'{encoded_target_name}.npy'), encoded_target)\n",
    "\n",
    "            with open(os.path.join(targets_save_path, 'targets.pickle'), 'wb') as f:\n",
    "                pickle.dump(self.targets, f)\n",
    "                \n",
    "            with open(os.path.join(targets_save_path, 'group_ids.pickle'), 'wb') as f:\n",
    "                pickle.dump(self.group_ids, f)\n",
    "\n",
    "            with open(os.path.join(targets_save_path, 'target_df.pickle'), 'wb') as f:\n",
    "                pickle.dump(self.target_df, f)\n",
    "\n",
    "            # self.target_df.to_csv(os.path.join(targets_save_path, 'target_df.csv'), index=False)    \n",
    "\n",
    "            with open(os.path.join(targets_save_path, 'encoded_targets_dict.pickle'), 'wb') as f:\n",
    "                pickle.dump(self.encoded_targets_dict, f)\n",
    "        elif save_to == 's3':\n",
    "            upload_object(self.targets, targets_save_path, 'targets.pickle')\n",
    "            upload_object(self.group_ids, targets_save_path, 'group_ids.pickle')\n",
    "            upload_object(self.target_df, targets_save_path, 'target_df.pickle')\n",
    "            upload_object(self.encoded_targets_dict, targets_save_path, 'encoded_targets_dict.pickle')\n",
    "        else:\n",
    "            raise ValueError(f\"save_to accept only 'file_system', or 's3' the following value was passed {save_to}\")\n",
    "        \n",
    "    def load_targets(self, targets_save_path, load_from='file_system'):\n",
    "        try:\n",
    "            if load_from == 'file_system':\n",
    "                with open(os.path.join(targets_save_path, 'targets.pickle'), 'rb') as f:\n",
    "                    self.targets = pickle.load(f)\n",
    "                    \n",
    "                with open(os.path.join(targets_save_path, 'group_ids.pickle'), 'rb') as f:\n",
    "                    self.group_ids = pickle.load(f)\n",
    "                    \n",
    "                with open(os.path.join(targets_save_path, 'target_df.pickle'), 'rb') as f:\n",
    "                    self.target_df = pickle.load(f)\n",
    "\n",
    "                # self.target_df = pd.read_csv(os.path.join(targets_save_path, 'target_df.csv'))   \n",
    "\n",
    "                # encoded_targets_dict must be loaded at the end, as setting target_df or targets will delete the encoded_targets_dict\n",
    "                with open(os.path.join(targets_save_path, 'encoded_targets_dict.pickle'), 'rb') as f:\n",
    "                    self.encoded_targets_dict = pickle.load(f)    \n",
    "            elif load_from == 's3':\n",
    "                self.targets = download_object(targets_save_path, 'targets.pickle')\n",
    "                self.group_ids = download_object(targets_save_path, 'group_ids.pickle')\n",
    "                self.target_df = download_object(targets_save_path, 'target_df.pickle')\n",
    "                self.encoded_targets_dict = download_object(targets_save_path, 'encoded_targets_dict.pickle')\n",
    "            else:\n",
    "                raise ValueError(f\"load_from accept only 'file_system', or 's3' the following value was passed {load_from}\")       \n",
    "            # self.encoded_targets_dict = {\n",
    "            #     os.path.splitext(encoded_target_path)[0]: np.load(os.path.join(targets_save_path, 'encoded_targets_dict', encoded_target_path))\n",
    "            #     for encoded_target_path in os.listdir(os.path.join(targets_save_path, 'encoded_targets_dict'))\n",
    "            # }\n",
    "        except FileNotFoundError as e:\n",
    "            raise FileNotFoundError(f'a target file was not found {e}')\n",
    "        \n",
    "    \n",
    "    def match(\n",
    "        self, \n",
    "        source: Union[List[str], pd.DataFrame], \n",
    "        source_mapping: Optional[Union[str, List]]=None, \n",
    "        topn: Optional[int]=1, \n",
    "        threshold: Optional[float]=0.5, \n",
    "        batch_size: Optional[int]=128\n",
    "    ) -> pd.DataFrame:\n",
    "        '''\n",
    "        Main match function. return only the top candidate for every source string.\n",
    "        parameters:\n",
    "            source: dataframe or list of input texts to find closest match for.\n",
    "            source_mapping (partially optional) *must be specified for dataframe matching*: a list with each element being a tuple with the following three values (the target column name, source column name, the weight for this match), if a string is passed it and one target was only passed it will be mapped to the that target, with a the full weight of 1.0, note that the the overall weights must equal 1.0.\n",
    "            topn: number of matches to return.\n",
    "            threshold: the lowest threeshold to ignore matches below it.\n",
    "            batch_size: the size of the batch in inputs to match with targets (to limit space usage).\n",
    "        returns:\n",
    "            a data frame with 3 columns (source, target, score), and two extra columns for each extra match (target_2, score_2 ...)\n",
    "        ''' \n",
    "        if isinstance(source, pd.DataFrame) and not hasattr(self, 'target_df'):\n",
    "            msg = 'if target is a dataframe source must also be a dataframe'\n",
    "            raise TypeError(msg)\n",
    "\n",
    "        if len(self.targets) == 1 and isinstance(source_mapping, str):\n",
    "            source_mapping = [(list(self.targets.keys())[0], source_mapping, 1)]\n",
    "\n",
    "        if isinstance(source, pd.DataFrame):\n",
    "            source = source.copy(deep=True)\n",
    "\n",
    "            overall_weight = 0.0\n",
    "            for _, source_col, weight in source_mapping:\n",
    "                if source_col not in source.columns:\n",
    "                    msg = f'the following source_col ({source_col}) not found in source DataFrame cloumns'\n",
    "                    raise KeyError(msg)\n",
    "                else:\n",
    "                    source.loc[:, source_col] = source[source_col].fillna('')\n",
    "                overall_weight += weight\n",
    "                \n",
    "            if overall_weight != 1.0:\n",
    "                msg = f'the sum of the provided weights must equal 1.0, the provided weights sum is: {overall_weight}'\n",
    "                raise ValueError(msg)\n",
    "            \n",
    "            self.source_df = source.reset_index(drop=True)\n",
    "            sources = {source_col: [\n",
    "                    preprocess(\n",
    "                    sent, \n",
    "                    self.replace_file_path,\n",
    "                    self.remove_punct, \n",
    "                    self.separate_punct,\n",
    "                    self.remove_stop_words, \n",
    "                    self.stemm, \n",
    "                    self.separate_numbers,\n",
    "                    self.lang)  \n",
    "                for sent in source[source_col].tolist()] \n",
    "                for _, source_col, __ in source_mapping\n",
    "            }\n",
    "        elif isinstance(source, list):\n",
    "            source = pd.DataFrame({'source': source})\n",
    "            self.source_df = source\n",
    "            if not source_mapping:\n",
    "                if len(self.targets) > 1:\n",
    "                    msg = 'there are multiple target columns to map with the source, please provide a costume source_mapping, or adjust the target columns to one specific columns'\n",
    "                    raise ValueError(msg)\n",
    "                source_mapping = [(list(self.targets.keys())[0], 'source', 1)]\n",
    "            sources = {source_col: [\n",
    "                    preprocess(\n",
    "                    sent, \n",
    "                    self.replace_file_path,\n",
    "                    self.remove_punct, \n",
    "                    self.separate_punct,\n",
    "                    self.remove_stop_words, \n",
    "                    self.stemm, \n",
    "                    self.separate_numbers,\n",
    "                    self.lang)  \n",
    "                for sent in source[source_col].tolist()] \n",
    "            for _, source_col, __ in source_mapping\n",
    "            }\n",
    "        else:\n",
    "            msg = f'source must be a dataframe or a list, instead a source of type {str(type(source))} was passed'\n",
    "            raise TypeError(msg)\n",
    "        # else:\n",
    "        #     self.source_df = pd.DataFrame({'source': source})\n",
    "        #     sources = {'source': [preprocess(sent, self.remove_punct, self.remove_stop_words, self.stemm, self.lang) for sent in source]}\n",
    "\n",
    "        if not hasattr(self, 'encoded_targets_dict'):\n",
    "            raise ValueError('The encoded_targets_dict was not defined, please use one of the following functions to initialize the encoded_targets_dict: (init_targets, load_targets)')\n",
    "        else:\n",
    "            encoded_targets_dict = self.encoded_targets_dict\n",
    "\n",
    "        inputs_length = len(list(sources.values())[0])\n",
    "        targets_length = len(list(self.targets.values())[0])\n",
    "\n",
    "        top_cosine = np.full((inputs_length, topn), None)\n",
    "        match_idxs = np.full((inputs_length, topn), None)\n",
    "\n",
    "        if self.show_prograss:\n",
    "            print('matching prograss:')\n",
    "\n",
    "        for i in tqdm(range(0, inputs_length, batch_size), disable=(not self.show_prograss)):\n",
    "            encoded_inputs = {(k): (self.model.encode(v[i:i+batch_size], batch_size=self.encode_batch, normalize_embeddings=True)) for k, v in sources.items()} # encode the inputs\n",
    "            batch_inputs_length = len(list(encoded_inputs.values())[0])\n",
    "            # encoded_inputs = self.model.encode(self.source_names[i:i+batch_size], batch_size=self.encode_batch, normalize_embeddings=True) # encode the inputs\n",
    "    \n",
    "            batch_top_cosine, batch_match_idxs = self.max_cosine_sim(encoded_inputs, encoded_targets_dict, source_mapping , topn, threshold, batch_inputs_length, targets_length)\n",
    "            top_cosine[i:i+batch_size, :] = batch_top_cosine\n",
    "            match_idxs[i:i+batch_size, :] = batch_match_idxs\n",
    "        \n",
    "        match_output = self._make_matchdf(top_cosine, match_idxs, inputs_length)\n",
    "\n",
    "        return match_output\n",
    "\n",
    "\n",
    "    def max_cosine_sim(self, encoded_inputs, encoded_targets_dict, source_mapping, topn, threshold, inputs_length, targets_length):\n",
    "        scores = np.zeros((inputs_length, targets_length), dtype=np.float32) # initialize with zeros\n",
    "  \n",
    "        for combinition_target, combinition_input, weight in source_mapping:\n",
    "            if len(encoded_inputs[combinition_input].shape) == 1:\n",
    "                encoded_inputs[combinition_input] = np.expand_dims(encoded_inputs[combinition_input], axis=0)\n",
    "\n",
    "            if len(encoded_targets_dict[combinition_target].shape) == 1:\n",
    "                encoded_targets_dict[combinition_target] = np.expand_dims(encoded_targets_dict[combinition_target], axis=0)\n",
    "\n",
    "            scores += np.matmul(encoded_inputs[combinition_input], encoded_targets_dict[combinition_target].T) * weight\n",
    "\n",
    "        if self.group_ids is None:\n",
    "            max_matches = min((targets_length-1, topn))\n",
    "        else:\n",
    "            max_matches = min((targets_length-1, topn * Counter(self.group_ids).most_common()[0][1]))\n",
    "        \n",
    "        top_sorted_idxs = np.argpartition(scores, -max_matches, axis=1)[:, -max_matches:] \n",
    "        \n",
    "        # resort the result as the partition sort doesn't completly sort the result\n",
    "        for i, idxs in enumerate(top_sorted_idxs):\n",
    "            top_sorted_idxs[i, :] = top_sorted_idxs[i, np.argsort(-scores[i, idxs])]\n",
    "\n",
    "        max_cosines = np.full((inputs_length, topn), None)\n",
    "        match_idxs = np.full((inputs_length, topn), None)\n",
    "            \n",
    "        # loop over top results to extract the index, target, and score for each match\n",
    "        if self.group_ids is not None:\n",
    "            for i, row in enumerate(top_sorted_idxs):\n",
    "                column_id = 0\n",
    "                previous_group_id = float('inf')\n",
    "                for highest_score_idx in row:\n",
    "                    if column_id >= topn or scores[i, highest_score_idx] < threshold:\n",
    "                        break\n",
    "                    if self.group_ids[highest_score_idx] == previous_group_id:\n",
    "                        continue\n",
    "                    match_idxs[i, column_id] = highest_score_idx\n",
    "                    max_cosines[i, column_id] = scores[i, highest_score_idx]\n",
    "                    \n",
    "                    column_id += 1\n",
    "                    previous_group_id = self.group_ids[highest_score_idx]\n",
    "        else:\n",
    "            for i, row in enumerate(top_sorted_idxs):\n",
    "                column_id = 0\n",
    "                for highest_score_idx in row:\n",
    "                    if column_id >= topn or scores[i, highest_score_idx] < threshold:\n",
    "                        break\n",
    "                    match_idxs[i, column_id] = highest_score_idx\n",
    "                    max_cosines[i, column_id] = scores[i, highest_score_idx]\n",
    "                    \n",
    "                    column_id += 1\n",
    "                    \n",
    "        return max_cosines, match_idxs\n",
    "\n",
    "\n",
    "    def _make_matchdf(self, top_cosine, match_idxs, inputs_length)-> pd.DataFrame:\n",
    "        ''' Build dataframe for result return '''\n",
    "        arr_temp = np.full((inputs_length, len(self.target_df.columns)+1), None)\n",
    "\n",
    "        for i, (match_idx, score) in enumerate(zip(match_idxs.T[0], top_cosine.T[0])):\n",
    "            if match_idx in self.target_df.index:\n",
    "                    temp = self.target_df.iloc[match_idx].tolist()\n",
    "                    temp.insert(0, score)\n",
    "                    arr_temp[i, :] = temp\n",
    "\n",
    "        cols = self.target_df.columns.tolist() \n",
    "        cols.insert(0, 'score_1')\n",
    "        match_df= pd.DataFrame(arr_temp, columns=cols)\n",
    "\n",
    "        # concat targets matches into one dataframe\n",
    "        for match_num in range(1, len(match_idxs.T)):\n",
    "            arr_temp = np.full((inputs_length, len(self.target_df.columns)+1), None)\n",
    "            for i, (match_idx, score) in enumerate(zip(match_idxs.T[match_num], top_cosine.T[match_num])):\n",
    "                if match_idx in self.target_df.index:\n",
    "                    temp = self.target_df.iloc[match_idx].tolist()\n",
    "                    temp.insert(0, score)\n",
    "                    arr_temp[i, :] = temp\n",
    "\n",
    "            cols = self.target_df.columns.tolist() \n",
    "            cols.insert(0, f'score_{match_num+1}')\n",
    "            df_temp= pd.DataFrame(arr_temp, columns=cols)\n",
    "            match_df = match_df.merge(df_temp, left_index=True, right_index=True, suffixes=(f'_{match_num}', f'_{match_num+1}'))\n",
    "\n",
    "        # merge matches with source\n",
    "        match_df = self.source_df.reset_index(drop=True).merge(match_df, left_index=True, right_index=True, suffixes=(f'_source', f'_target'))\n",
    "\n",
    "        return match_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.similarity_check.utils import _get_enviroment_variables, _get_s3_resource\n",
    "\n",
    "def get_stc_test_data():\n",
    "    X = pd.DataFrame({\n",
    "        'text': ['Cholera, a unspecified', 'remove test'],\n",
    "        'id': [1, 2],\n",
    "    }\n",
    "    )\n",
    "\n",
    "    y = pd.DataFrame({\n",
    "        'new_text': ['Cholera', 'stop the test', 'testing'],\n",
    "        'new_id': [1, 2, 3],\n",
    "        'tags': ['pos', 'neg', 'pos'],\n",
    "        'num': [10, 22, 40],\n",
    "        'day': [3, 5, 2],\n",
    "    }\n",
    "    )\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def test_stc_match_accuracy():\n",
    "    X, y = get_stc_test_data()\n",
    "\n",
    "    stc = sentence_tranformer_checker()\n",
    "    stc.init_targets(y, target_cols='new_text', target_group='tags')\n",
    "\n",
    "    df_match = stc.match(X, source_mapping=[('new_text', 'text', 1)])\n",
    "\n",
    "    assert df_match['new_text'].iloc[0] == 'Cholera', \"Cholera, a unspecified wasn't matched correctly\"\n",
    "    assert df_match['new_text'].iloc[1] == 'stop the test', \"remove test wasn't matched correctly\"\n",
    "\n",
    "\n",
    "def test_stc_match_only_include():\n",
    "    X, y = get_stc_test_data()\n",
    "\n",
    "    stc = sentence_tranformer_checker()\n",
    "    stc.init_targets(y, target_cols='new_text', target_group='tags', only_include=[])\n",
    "\n",
    "    df_match = stc.match(X, source_mapping=[('new_text', 'text', 1)], topn=10)\n",
    "    \n",
    "    assert len(df_match.columns) == 22, f\"the result match didn't include all top 10 matches details, it should include 22 columns but {len(df_match.columns)} columns was found\"\n",
    "\n",
    "\n",
    "def test_stc_match_topn():\n",
    "    X, y = get_stc_test_data()\n",
    "\n",
    "    stc = sentence_tranformer_checker()\n",
    "    stc.init_targets(y, target_cols='new_text', target_group='tags')\n",
    "\n",
    "    df_match = stc.match(X, source_mapping=[('new_text', 'text', 1)], topn=10)\n",
    "\n",
    "    assert len(df_match.columns) == 62, f\"the result match didn't include all top 10 matches details, it should include 62 columns but {len(df_match.columns)} columns was found\"\n",
    "\n",
    "\n",
    "def _targets_file_system_save():\n",
    "    X, y = get_stc_test_data()\n",
    "    stc = sentence_tranformer_checker()\n",
    "    stc.init_targets(y, target_cols='new_text', target_group='tags')\n",
    "    stc.save_targets('targets_test/')\n",
    "\n",
    "\n",
    "def test_stc_targets_save_file_system():\n",
    "    _targets_file_system_save()\n",
    "\n",
    "    assert os.path.exists('targets_test/targets.pickle'), \"'targets' was not saved\"\n",
    "    assert os.path.exists('targets_test/group_ids.pickle'), \"'group_ids' was not saved\"\n",
    "    assert os.path.exists('targets_test/target_df.pickle'), \"'target_df' was not saved\"\n",
    "    assert os.path.exists('targets_test/encoded_targets_dict.pickle'), \"'encoded_targets_dict' was not saved\"\n",
    "\n",
    "    for root, dirs, files in os.walk('targets_test/', topdown=False):\n",
    "        for name in files:\n",
    "            os.remove(os.path.join(root, name))\n",
    "        for name in dirs:\n",
    "            os.rmdir(os.path.join(root, name))\n",
    "\n",
    "\n",
    "def test_stc_targets_load_file_system():\n",
    "    _targets_file_system_save()\n",
    "\n",
    "    stc = sentence_tranformer_checker()\n",
    "    stc.load_targets('targets_test/')\n",
    "    \n",
    "    assert stc.targets is not None, \"'targets' was not loaded\"\n",
    "    assert stc.group_ids is not None, \"'group_ids' was not loaded\"\n",
    "    assert stc.target_df is not None, \"'target_df' was not loaded\"\n",
    "    assert stc.encoded_targets_dict is not None, \"'encoded_targets_dict' was not loaded\"\n",
    "\n",
    "    for root, dirs, files in os.walk('targets_test/', topdown=False):\n",
    "        for name in files:\n",
    "            os.remove(os.path.join(root, name))\n",
    "        for name in dirs:\n",
    "            os.rmdir(os.path.join(root, name))\n",
    "\n",
    "\n",
    "def _targets_s3_save():\n",
    "    endpoint, bucket_name, access_key, secret_key = _get_enviroment_variables()\n",
    "    s3  = _get_s3_resource(endpoint, access_key, secret_key)\n",
    "    X, y = get_stc_test_data()\n",
    "    stc = sentence_tranformer_checker()\n",
    "    stc.init_targets(y, target_cols='new_text', target_group='tags')\n",
    "    stc.save_targets('targets_test/', save_to='s3')\n",
    "\n",
    "    return s3, bucket_name\n",
    "\n",
    "\n",
    "def test_stc_targets_save_s3():\n",
    "    s3, bucket_name = _targets_s3_save()\n",
    "\n",
    "    assert s3.Object(bucket_name,'targets_test/targets.pickle').get()['ResponseMetadata']['HTTPStatusCode'] == 200\n",
    "    assert s3.Object(bucket_name, 'targets_test/group_ids.pickle').get()['ResponseMetadata']['HTTPStatusCode'] == 200\n",
    "    assert s3.Object(bucket_name, 'targets_test/targets.pickle').get()['ResponseMetadata']['HTTPStatusCode'] == 200\n",
    "    assert s3.Object(bucket_name, 'targets_test/targets.pickle').get()['ResponseMetadata']['HTTPStatusCode'] == 200\n",
    "    \n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    bucket.objects.filter(Prefix=\"targets_test/\").delete()\n",
    "\n",
    "\n",
    "def test_stc_targets_load_s3():\n",
    "    s3, bucket_name = _targets_s3_save()\n",
    "\n",
    "    stc = sentence_tranformer_checker()\n",
    "    stc.load_targets('targets_test/', load_from='s3')\n",
    "    \n",
    "    assert stc.targets is not None, \"'targets' was not loaded\"\n",
    "    assert stc.group_ids is not None, \"'group_ids' was not loaded\"\n",
    "    assert stc.target_df is not None, \"'target_df' was not loaded\"\n",
    "    assert stc.encoded_targets_dict is not None, \"'encoded_targets_dict' was not loaded\"\n",
    "\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    bucket.objects.filter(Prefix=\"targets_test/\").delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stc_match_accuracy()\n",
    "test_stc_match_only_include()\n",
    "test_stc_match_topn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stc_targets_save_file_system()\n",
    "test_stc_targets_load_file_system()\n",
    "test_stc_targets_save_s3()\n",
    "test_stc_targets_load_s3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_stc_test_data()\n",
    "stc = sentence_tranformer_checker()\n",
    "stc.init_targets(y, target_cols='new_text',target_group='tags', only_include=['new_id'])\n",
    "stc.save_targets('targets_test/', save_to='s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '1698830187308595',\n",
       "  'HostId': '12171445',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Wed, 01 Nov 2023 09:16:27 GMT',\n",
       "   'connection': 'KEEP-ALIVE',\n",
       "   'server': 'STCObjectStorage',\n",
       "   'x-amz-request-id': '1698830187308595',\n",
       "   'x-amz-id-2': '12171445',\n",
       "   'content-length': '66',\n",
       "   'x-ntap-sg-trace-id': 'd84951858fcd3c31',\n",
       "   'etag': '\"a2200e8d7eb6fdb9de0d6ca1f4103807\"',\n",
       "   'content-type': 'binary/octet-stream',\n",
       "   'last-modified': 'Wed, 01 Nov 2023 09:11:59 GMT',\n",
       "   'accept-ranges': 'bytes',\n",
       "   'strict-transport-security': 'max-age=16070400; includeSubDomains'},\n",
       "  'RetryAttempts': 0},\n",
       " 'AcceptRanges': 'bytes',\n",
       " 'LastModified': datetime.datetime(2023, 11, 1, 9, 11, 59, tzinfo=tzutc()),\n",
       " 'ContentLength': 66,\n",
       " 'ETag': '\"a2200e8d7eb6fdb9de0d6ca1f4103807\"',\n",
       " 'ContentType': 'binary/octet-stream',\n",
       " 'Metadata': {},\n",
       " 'Body': <botocore.response.StreamingBody at 0x7faa995cb550>}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ['test', 'remove test']\n",
    "y =  ['tests', 'stop the test', 'testing']\n",
    "\n",
    "### arabic example:\n",
    "# X = ['حذف الاختبار', 'اختبار']\n",
    "# y =  ['اختبارات', 'ايقاف الاختبار']\n",
    "# st = sentence_tranformer(lang='ar')\n",
    "st = sentence_tranformer_checker()\n",
    "\n",
    "st.init_targets(X)\n",
    "match_df = st.match(y, topn=4, threshold=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>score_1</th>\n",
       "      <th>target_1</th>\n",
       "      <th>score_2</th>\n",
       "      <th>target_2</th>\n",
       "      <th>score_3</th>\n",
       "      <th>target_3</th>\n",
       "      <th>score_4</th>\n",
       "      <th>target_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tests</td>\n",
       "      <td>0.922843</td>\n",
       "      <td>test</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stop the test</td>\n",
       "      <td>0.728872</td>\n",
       "      <td>remove test</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>testing</td>\n",
       "      <td>0.908599</td>\n",
       "      <td>test</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          source   score_1     target_1 score_2 target_2 score_3 target_3  \\\n",
       "0          tests  0.922843         test    None     None    None     None   \n",
       "1  stop the test  0.728872  remove test    None     None    None     None   \n",
       "2        testing  0.908599         test    None     None    None     None   \n",
       "\n",
       "  score_4 target_4  \n",
       "0    None     None  \n",
       "1    None     None  \n",
       "2    None     None  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame({\n",
    "    'text': ['Cholera, a unspecified', 'remove test'],\n",
    "    'id': [1, 2],\n",
    "}\n",
    ")\n",
    "\n",
    "y = pd.DataFrame({\n",
    "    'new_text': ['Cholera', 'stop the test', 'testing'],\n",
    "    'new_id': [1, 2, 3],\n",
    "    'tags': ['pos', 'neg', 'pos'],\n",
    "    'num': [10, 22, 40],\n",
    "    'day': [3, 5, 2],\n",
    "}\n",
    ")\n",
    "\n",
    "st = sentence_tranformer_checker()\n",
    "st.init_targets(y, target_cols='new_text',target_group='tags', only_include=['new_id'])\n",
    "match_df = st.match(X, source_mapping=[('new_text','text', 1)], topn=4, threshold=0.6, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>new_text</th>\n",
       "      <th>new_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cholera</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stop the test</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>testing</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        new_text  new_id\n",
       "0        Cholera       1\n",
       "1  stop the test       2\n",
       "2        testing       3"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'new_text': array([[-0.00068197, -0.01440026, -0.00045839, ...,  0.05169116,\n",
       "          0.10978685, -0.01774278],\n",
       "        [ 0.00214827,  0.06697568, -0.00280759, ..., -0.01590737,\n",
       "          0.06488785,  0.00502195],\n",
       "        [-0.01549809,  0.01749647, -0.03467191, ...,  0.02056227,\n",
       "          0.02446756, -0.04901983]], dtype=float32)}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.encoded_targets_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>score_1</th>\n",
       "      <th>new_text_1</th>\n",
       "      <th>new_id_1</th>\n",
       "      <th>score_2</th>\n",
       "      <th>new_text_2</th>\n",
       "      <th>new_id_2</th>\n",
       "      <th>score_3</th>\n",
       "      <th>new_text_3</th>\n",
       "      <th>new_id_3</th>\n",
       "      <th>score_4</th>\n",
       "      <th>new_text_4</th>\n",
       "      <th>new_id_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cholera, a unspecified</td>\n",
       "      <td>1</td>\n",
       "      <td>0.90144</td>\n",
       "      <td>Cholera</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>remove test</td>\n",
       "      <td>2</td>\n",
       "      <td>0.728872</td>\n",
       "      <td>stop the test</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     text  id   score_1     new_text_1 new_id_1 score_2  \\\n",
       "0  Cholera, a unspecified   1   0.90144        Cholera        1    None   \n",
       "1             remove test   2  0.728872  stop the test        2    None   \n",
       "\n",
       "  new_text_2 new_id_2 score_3 new_text_3 new_id_3 score_4 new_text_4 new_id_4  \n",
       "0       None     None    None       None     None    None       None     None  \n",
       "1       None     None    None       None     None    None       None     None  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.match(X, source_mapping=[('new_text','text', 1)], topn=4, threshold=0.6, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame({\n",
    "    'text': ['Cholera, a unspecified', 'remove test'],\n",
    "    'id': [1, 2],\n",
    "}\n",
    ")\n",
    "\n",
    "y = pd.DataFrame({\n",
    "    'new_text': ['Cholera', 'stop the test', 'testing'],\n",
    "    'new_id': [1, 2, 3],\n",
    "    'tags': ['pos', 'neg', 'pos'],\n",
    "    'num': [10, 22, 40],\n",
    "    'day': [3, 5, 2],\n",
    "}\n",
    ")\n",
    "\n",
    "st = sentence_tranformer_checker()\n",
    "st.init_targets(y, target_cols='new_text',target_group='tags', only_include=['new_id'])\n",
    "match_df = st.match(X, source_mapping=[('new_text','text', 1)], topn=4, threshold=0.6, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>score_1</th>\n",
       "      <th>new_text_1</th>\n",
       "      <th>new_id_1</th>\n",
       "      <th>score_2</th>\n",
       "      <th>new_text_2</th>\n",
       "      <th>new_id_2</th>\n",
       "      <th>score_3</th>\n",
       "      <th>new_text_3</th>\n",
       "      <th>new_id_3</th>\n",
       "      <th>score_4</th>\n",
       "      <th>new_text_4</th>\n",
       "      <th>new_id_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cholera, a unspecified</td>\n",
       "      <td>1</td>\n",
       "      <td>0.868838</td>\n",
       "      <td>Cholera</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>remove test</td>\n",
       "      <td>2</td>\n",
       "      <td>0.728872</td>\n",
       "      <td>stop the test</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     text  id   score_1     new_text_1 new_id_1 score_2   \n",
       "0  Cholera, a unspecified   1  0.868838        Cholera        1    None  \\\n",
       "1             remove test   2  0.728872  stop the test        2    None   \n",
       "\n",
       "  new_text_2 new_id_2 score_3 new_text_3 new_id_3 score_4 new_text_4 new_id_4  \n",
       "0       None     None    None       None     None    None       None     None  \n",
       "1       None     None    None       None     None    None       None     None  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('test.pickle', 'wb') as file:\n",
    "    pickle.dump(st, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.pickle', 'rb') as file:\n",
    "    st = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_df = st.match(X, source_mapping=[('new_text','text', 1)], topn=4, threshold=0.6, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>score_1</th>\n",
       "      <th>new_text_1</th>\n",
       "      <th>new_id_1</th>\n",
       "      <th>score_2</th>\n",
       "      <th>new_text_2</th>\n",
       "      <th>new_id_2</th>\n",
       "      <th>score_3</th>\n",
       "      <th>new_text_3</th>\n",
       "      <th>new_id_3</th>\n",
       "      <th>score_4</th>\n",
       "      <th>new_text_4</th>\n",
       "      <th>new_id_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cholera, a unspecified</td>\n",
       "      <td>1</td>\n",
       "      <td>0.868838</td>\n",
       "      <td>Cholera</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>remove test</td>\n",
       "      <td>2</td>\n",
       "      <td>0.728872</td>\n",
       "      <td>stop the test</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     text  id   score_1     new_text_1 new_id_1 score_2   \n",
       "0  Cholera, a unspecified   1  0.868838        Cholera        1    None  \\\n",
       "1             remove test   2  0.728872  stop the test        2    None   \n",
       "\n",
       "  new_text_2 new_id_2 score_3 new_text_3 new_id_3 score_4 new_text_4 new_id_4  \n",
       "0       None     None    None       None     None    None       None     None  \n",
       "1       None     None    None       None     None    None       None     None  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
