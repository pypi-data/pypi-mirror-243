# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_atyp.ipynb.

# %% auto 0
__all__ = ['VidsInfo', 'CellState', 'HiddenState', 'RNNState', 'GRUState', 'LSTMStates', 'RNNStateQ', 'GRUStateQ', 'LSTMStatesQ',
           'RecurrentStates', 'RecurrentStatesQ', 'HiddenStates', 'HiddenStatesQ', 'BaseRecurrentKeywords',
           'LSTMKeywords', 'RNNKeywords', 'RecurrentKeywords', 'RecurrentAutoEncoderKeywords', 'SubVidInfo', 'VidInfo',
           'MaskInfo']

# %% ../nbs/01_atyp.ipynb 6
from pathlib import Path

# %% ../nbs/01_atyp.ipynb 8
from typing import Tuple, Union, Optional, TypeAlias, TypedDict, ForwardRef

# %% ../nbs/01_atyp.ipynb 11
#| export


# %% ../nbs/01_atyp.ipynb 13
#| export


# %% ../nbs/01_atyp.ipynb 15
#| export


# %% ../nbs/01_atyp.ipynb 17
#| export


# %% ../nbs/01_atyp.ipynb 19
from atyp import Tensor, XYPos, BBox, Rect, DTypeQ, DeviceQ, Size

# %% ../nbs/01_atyp.ipynb 21
#| export


# %% ../nbs/01_atyp.ipynb 24
class BaseRecurrentKeywords(TypedDict):
    '''Typed dictionary for specifying base keywords for recurrent neural network layers.

    Attributes
    ----------
    input_size : int
        The number of expected features in the input.

    hidden_size : int
        The number of features in the hidden state.

    num_layers : int
        Number of recurrent layers.

    bias : bool
        If False, then the layer does not use bias weights.

    batch_first : bool
        If True, then the input and output tensors are provided as (batch, seq, feature).

    dropout : float
        If non-zero, introduces a dropout layer on the outputs of each layer except the last layer.

    bidirectional : bool
        If True, becomes a bidirectional layer.

    device : DeviceQ
        The device on which to train the model.

    dtype : DTypeQ
        The data type of the model parameters.

    Examples
    --------
    >>> base_rnn_keywords = BaseRecurrentKeywords(
    ...     input_size=64, hidden_size=128, num_layers=2, bias=True, 
    ...     batch_first=True, dropout=0.5, bidirectional=False, device='cuda', 
    ...     dtype=torch.float32
    ... )
    '''
    input_size: int
    hidden_size: int
    num_layers: int
    
    bias: bool
    batch_first: bool 
    dropout: float 
    bidirectional: bool 
    
    device: DeviceQ  
    dtype: DTypeQ 

# %% ../nbs/01_atyp.ipynb 25
class LSTMKeywords(BaseRecurrentKeywords):
    '''Typed dictionary for specifying keywords specific to LSTM layers, extending `BaseRecurrentKeywords`.

    Attributes
    ----------
    proj_size : int
        The size of the projection layer.

    Examples
    --------
    >>> lstm_keywords = LSTMKeywords(
    ...     input_size=64, hidden_size=128, num_layers=2, proj_size=64, bias=True, 
    ...     batch_first=True, dropout=0.5, bidirectional=False, device='cuda', 
    ...     dtype=torch.float32
    ... )
    '''
    proj_size: int  
    
class RNNKeywords(BaseRecurrentKeywords):    
    '''Typed dictionary for specifying keywords specific to RNN layers, extending `BaseRecurrentKeywords`.

    Attributes
    ----------
    nonlinearity : NonLinearity
        The non-linearity to use ('relu' or 'tanh').

    Examples
    --------
    >>> rnn_keywords = RNNKeywords(
    ...     input_size=64, hidden_size=128, num_layers=2, nonlinearity=NonLinearity.Tanh, 
    ...     bias=True, batch_first=True, dropout=0.5, bidirectional=False, device='cuda', 
    ...     dtype=torch.float32
    ... )
    '''
    nonlinearity: ForwardRef('NonLinearity', module='ptrc.enum', is_class=True)

# %% ../nbs/01_atyp.ipynb 26
class RecurrentKeywords(LSTMKeywords, RNNKeywords): 
    '''Typed dictionary for specifying keywords for recurrent neural network layers, combining `LSTMKeywords` and `RNNKeywords`.

    Examples
    --------
    >>> recurrent_keywords = RecurrentKeywords(
    ...     input_size=64, hidden_size=128, num_layers=2, proj_size=64, nonlinearity=NonLinearity.Tanh, 
    ...     bias=True, batch_first=True, dropout=0.5, bidirectional=False, device='cuda', 
    ...     dtype=torch.float32
    ... )
    '''

# %% ../nbs/01_atyp.ipynb 27
class RecurrentAutoEncoderKeywords(RecurrentKeywords):
    '''Typed dictionary for specifying keywords for recurrent autoencoder models, extending `RecurrentKeywords`.

    Attributes
    ----------
    output_size : int
        The size of the output layer.
        
    kind : RecurrentLayer
        The type of recurrent layer to use.

    Examples
    --------
    >>> recurrent_ae_keywords = RecurrentAutoEncoderKeywords(
    ...     input_size=64, hidden_size=128, num_layers=2, proj_size=64, output_size=32, 
    ...     kind=RecurrentLayer.LSTM, nonlinearity=NonLinearity.Tanh, bias=True, batch_first=True, 
    ...     dropout=0.5, bidirectional=False, device='cuda', dtype=torch.float32
    ... )
    '''
    output_size: int
    kind: ForwardRef('RecurrentLayer', module='ptrc.enum', is_class=True)

# %% ../nbs/01_atyp.ipynb 30
class SubVidInfo(TypedDict):
    '''Subvideo Information from an AVI file'''
    pos: XYPos
    '''The (row, col) position of the subvideo in the grid'''
    bbox: BBox
    '''The bounding box of the subvideo in the grid'''
    subidx: int
    '''The flattened grid index of the subvideo'''
    subkey: str
    '''The label the subvideo'''

# %% ../nbs/01_atyp.ipynb 32
class VidInfo(TypedDict):
    '''AVI file information'''
    vidpxs: Rect
    '''The frame size (width, height) of the video e.g. (1200, 1200)'''
    subpxs: Rect
    '''The standardized subvideo size (sub-width, sub-height) e.g. (400, 400)'''
    values: list[dict[str, SubVidInfo]]
    '''The subvideo information'''
    source: Path
    '''The source AVI file'''

# %% ../nbs/01_atyp.ipynb 34
VidsInfo: TypeAlias = dict[Path, VidInfo]
'''Dictionary of (path, vidinfo) pairs''';

# %% ../nbs/01_atyp.ipynb 37
class MaskInfo(TypedDict):
    '''Typed dictionary for specifying mask information.

    Attributes
    ----------
    size : tuple
        The size of the mask.

    masks : dict[str, BBox]
        A dictionary mapping mask names to their values.

    value : tuple
        The value to be applied for the mask.

    Examples
    --------
    >>> white = (255, 255, 255)
    >>> mask_bbox = ((0, 5), (10, 15))
    >>> mask_info = MaskInfo(size=(32, 32), masks={'example': mask_bbox}, value=white)
    '''
    size: Size
    '''The size of the mask.'''
    masks: dict[str, BBox]
    '''A dictionary mapping mask names to their values.''';
    value: tuple
    '''The value to be applied for all masks in this dictionary.''';

# %% ../nbs/01_atyp.ipynb 39
CellState: TypeAlias = Tensor
'''Type alias for representing the cell state in LSTM layers (`torch.Tensor`).

Represents the cell state tensor in LSTM layers, which carries information 
across cell sequences during the operation of the LSTM.
''';

HiddenState: TypeAlias = Tensor
'''Type alias for representing the hidden state in RNN layers (`torch.Tensor`).

Represents the hidden state tensor in RNN layers, including LSTM and GRU, 
which is passed across different time steps of the RNN operation.
''';

RNNState: TypeAlias = HiddenState
'''Type alias for the hidden state in a simple RNN layer (`torch.Tensor`).

Represents the hidden state in a simple RNN (Recurrent Neural Network) layer, 
holding the information passed across time steps.
''';

GRUState: TypeAlias = HiddenState
'''Type alias for the hidden state in a GRU (Gated Recurrent Unit) layer  (`torch.Tensor`).

Represents the hidden state in a GRU layer, which is a more complex RNN 
architecture that includes gating mechanisms.
''';

LSTMStates: TypeAlias = Tuple[HiddenState, CellState]
'''Type alias for the combined hidden and cell states in an LSTM layer (`tuple[torch.Tensor, torch.Tensor]`).

Represents a tuple containing both the hidden state and cell state in an LSTM 
(Long Short-Term Memory) layer, which are used for carrying information across 
time steps and sequences.
''';

RNNStateQ: TypeAlias = Optional[RNNState]
'''Type alias for an optional hidden state in a simple RNN layer (`Optional[torch.Tensor]`).

Represents an optional hidden state for a simple RNN layer, which may or may not 
be provided depending on the context of the RNN operation.
''';

GRUStateQ: TypeAlias = Optional[GRUState]
'''Type alias for an optional hidden state in a GRU layer (`Optional[torch.Tensor]`).

Represents an optional hidden state for a GRU (Gated Recurrent Unit) layer, 
which may be omitted in certain use cases or initial states.
''';

LSTMStatesQ: TypeAlias = Optional[LSTMStates]
'''Type alias for optional combined hidden and cell states in an LSTM layer
(`Optional[tuple[torch.Tensor, torch.Tensor]]`).

Represents an optional tuple of hidden and cell states for an LSTM (Long Short-Term Memory) 
layer, which can be omitted in initial steps or specific applications.
''';

RecurrentStates: TypeAlias = Union[RNNState, GRUState, LSTMStates]
'''Type alias for representing the hidden states of various recurrent layers
`torch.Tensor | tuple[torch.Tensor, torch.Tensor`.

Encapsulates the hidden states for different types of recurrent layers 
(simple RNN, GRU, and LSTM), providing a unified type for handling 
recurrent layer states.
'''

RecurrentStatesQ: TypeAlias = Optional[RecurrentStates]
'''Type alias for an optional representation of hidden states in recurrent layers
`Optional[torch.Tensor | tuple[torch.Tensor, torch.Tensor]`.

Represents an optional type that can encapsulate hidden states of various 
recurrent layers, allowing for flexibility in specifying these states.
'''

HiddenStates: TypeAlias = RecurrentStates
'''Type alias for hidden states in any recurrent neural network layers
`torch.Tensor | tuple[torch.Tensor, torch.Tensor`.

A general type for representing hidden states in RNN, GRU, or LSTM layers,
facilitating a consistent interface for these various architectures.
'''

HiddenStatesQ: TypeAlias = RecurrentStatesQ
'''Type alias for an optional hidden state in recurrent neural network layers
`Optional[torch.Tensor | tuple[torch.Tensor, torch.Tensor]`.

Provides an optional type for representing hidden states in RNN, GRU, or LSTM 
layers, which can be used in contexts where the state may not be initially defined.
'''

# %% ../nbs/01_atyp.ipynb 41
#| export
