# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/40_rcur.ipynb.

# %% auto 0
__all__ = ['RecurrentAutoEncoderMixin', 'Encoder', 'Decoder', 'AutoEncoder']

# %% ../nbs/40_rcur.ipynb 6
import logging
from functools import wraps

# %% ../nbs/40_rcur.ipynb 8
#| export

# %% ../nbs/40_rcur.ipynb 11
#| export

# %% ../nbs/40_rcur.ipynb 13
try: import numpy as np
except ImportError: ...

# %% ../nbs/40_rcur.ipynb 15
try: import torch, torch.nn as nn    
except ImportError: 
    class torch:
        cat = type
    class nn:
        Module = type; Linear = type; ReLU = type

# %% ../nbs/40_rcur.ipynb 17
#| export


# %% ../nbs/40_rcur.ipynb 19
from atyp import IntQ, Tensor, TensorQ, DeviceQ, DTypeQ
from chck import isint, notnone
from fpos import val1st
from putl import dropkeys, deconstruct

# %% ../nbs/40_rcur.ipynb 21
from .atyp import RecurrentStatesQ
from .logr import EncoderLogMessage, DecoderLogMessage
from .enum import NonLinearity, RecurrentLayer, InitMethod
from .kwds import recurrent_kwds, recurrent_encoder_kwds, recurrent_decoder_kwds
from .util import as4d, batches, channels
from .init import directed_layers, init_r0, init_rn

# %% ../nbs/40_rcur.ipynb 23
class RecurrentAutoEncoderMixin:
    '''
    Mixin class for recurrent autoencoder components, providing parameter initialization.

    This class provides methods to initialize parameters for encoder and decoder components
    of a recurrent autoencoder.

    Methods
    -------
    init_params(...):
        Initialize parameters for the recurrent layer.

    prep_inputs(x, h0, c0, hcell, hsize):
        Prepare inputs for the recurrent layer, including hidden and cell states.
    '''
    def init_params(
        self, 
        
        input_size: int, 
        hidden_size: int,
        num_layers: int = 1, 
        bias: bool = True,
        batch_first: bool = True, 
        dropout: float = 0.2, 
        bidirectional: bool = False,
        proj_size: int = 0, 
        nonlinearity: NonLinearity = NonLinearity.Tanh, 
        device: DeviceQ = None, 
        dtype: DTypeQ = None, 
        
        kind: RecurrentLayer = RecurrentLayer.LSTM, 
        
        decoder: bool = False,
        
        init: InitMethod = InitMethod.orthogonal,
        gain: float = np.sqrt(2),

        make_relu: bool = False,
        make_linear: bool = False,
        linear_size: IntQ = None,
        logger: logging.Logger = None,
    ):
        '''
        Initialize parameters for a recurrent layer within an autoencoder.

        This method configures the parameters and auxiliary components like recurrent layers,
        activation functions, and linear transformation layers. It sets up the architecture based 
        on the specified configuration, catering to both encoder and decoder components.

        Parameters
        ----------
        input_size : int
            The number of expected features in the input `x`.

        hidden_size : int
            The number of features in the hidden state `h`.

        num_layers : int, optional
            Number of recurrent layers. Default is 1.

        bias : bool, optional
            If `False`, then the layer does not use bias weights. Default is `True`.

        batch_first : bool, optional
            If `True`, then the input and output tensors are provided as `(batch, seq, feature)`.
            Default is `True`.

        dropout : float, optional
            If non-zero, introduces a dropout layer on the outputs of each RNN layer except the last layer.
            Default is 0.2.

        bidirectional : bool, optional
            If `True`, becomes a bidirectional RNN. Default is `False`.

        proj_size : int, optional
            If > 0, will add a linear projection layer to the output of the RNN. Default is 0.

        nonlinearity : NonLinearity, optional
            The non-linearity to use. Default is `NonLinearity.Tanh`.

        device : DeviceQ, optional
            Device on which to allocate the layer's tensors. Default is `None`.

        dtype : DTypeQ, optional
            Data type for the layer's tensors. Default is `None`.

        kind : RecurrentLayer, optional
            Type of the recurrent layer (e.g., LSTM, GRU). Default is `RecurrentLayer.LSTM`.

        decoder : bool, optional
            Indicates if the setup is for the decoder part of the autoencoder. Default is `False`.

        init : InitMethod, optional
            The method used for weight initialization. Default is `InitMethod.orthogonal`.

        gain : float, optional
            Multiplier for initializing weights. Default is `sqrt(2)`.

        make_relu : bool, optional
            If `True`, adds a ReLU activation layer. Default is `False`.

        make_linear : bool, optional
            If `True`, adds a linear transformation layer. Default is `False`.

        linear_size : IntQ, optional
            Size of the linear layer, only used if `make_linear` is `True`. Default is `None`.

        logger : logging.Logger, optional
            Logger for logging messages. Default is `None`.

        Examples
        --------
        >>> encoder = Encoder(10, 20)
        >>> encoder.init_params(input_size=10, hidden_size=20, ...)

        See Also
        --------
        recurrent_kwds : Get recurrent layer parameters.
        RecurrentLayer : Enum for recurrent layer types.
        RecurrentLayer.get : Get recurrent layer based on type.
        InitMethod : Enum for parameter initialization methods.
        InitMethod.get : Get parameter initialization method.
        '''
        params = recurrent_kwds(
            input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, bias=bias, 
            batch_first=batch_first, dropout=dropout, bidirectional=bidirectional,
            proj_size=proj_size, nonlinearity=nonlinearity, device=device, dtype=dtype,
            kind=RecurrentLayer(kind).value
        )
        params = params if not decoder else recurrent_decoder_kwds(**params)
        
        self.params = dict(
            **params, kind=kind, decoder=decoder, init=init, gain=gain, 
            make_relu=make_relu, make_linear=make_linear, linear_size=linear_size, log=logger
        )
        for k, v in self.params.items(): setattr(self, k, v)

        self.rmod = RecurrentLayer(kind).get(**recurrent_encoder_kwds(**params))
        InitMethod(init).get(self.rmod.weight_ih_l0, gain=gain)
        InitMethod(init).get(self.rmod.weight_hh_l0, gain=gain)

        if make_relu: 
            self.relu = nn.ReLU()

        if make_linear:
            hsize = val1st(params, ('input_size', 'hidden_size'), default=0)
            osize = val1st(params, ('output_size', 'input_size', 'hidden_size'), default=0)
            self.full = nn.Linear(hsize, osize, bias=bias, device=device, dtype=dtype)
            InitMethod(init).get(self.full.weight, gain=gain)

    def prep_inputs(
        self, x: Tensor, h0: TensorQ = None, c0: TensorQ = None, 
        hcell: IntQ = None, hsize: IntQ = None
    ) -> (Tensor, RecurrentStatesQ):
        '''
        Prepare input tensors for the recurrent layer, including hidden and cell states.

        Parameters
        ----------
        x : Tensor
            Input tensor to the recurrent layer.

        h0 : TensorQ, optional
            Initial hidden state tensor, defaults to None.

        c0 : TensorQ, optional
            Initial cell state tensor (for LSTM), defaults to None.

        hcell : IntQ, optional
            Cell size for hidden states, defaults to None.

        hsize : IntQ, optional
            Size for hidden states, defaults to None.

        Returns
        -------
        (Tensor, RecurrentStatesQ)
            Prepared input tensor and tuple of hidden and cell states.

        Examples
        --------
        >>> encoder = Encoder(10, 20)
        >>> x, states = encoder.prep_inputs(torch.rand(5, 3, 10))


        See Also
        --------
        init_r0 : Initialize hidden and cell states.
        '''
        
        hcell = hcell if notnone(hcell) else getattr(self, 'hidden_size', None)
        hsize = hsize if notnone(hsize) else getattr(self, 'proj_size', None)
        
        if notnone(hsize) and not (hsize > 0): 
            hsize = hcell
        
        bsize = batches(x, self.batch_first)
        nlays = directed_layers(self.num_layers, self.bidirectional)        
        device = self.device if notnone(self.device) else x.device
        return init_r0(
            x, h0, c0, nlays, hcell, hsize, bsize, device, batch_first = True, bidirectional = None, 
            kind = RecurrentLayer(self.kind).value
        )

# %% ../nbs/40_rcur.ipynb 25
class Encoder(nn.Module, RecurrentAutoEncoderMixin):
    '''
    Encoder component of a recurrent autoencoder.

    Inherits from nn.Module and RecurrentAutoEncoderMixin, and provides methods specific
    to the encoder component of an autoencoder.

    Methods
    -------
    __init__(...):
        Initialize the encoder with specified parameters.
        
    latent(x, retlast):
        Compute the latent representation of the input.

    forward(x, retlast):
        Forward pass through the encoder.

    Examples
    --------
    >>> encoder = Encoder(input_size=10, hidden_size=20)
    >>> latent = encoder.latent(torch.rand(5, 3, 10))
    '''
    
    @wraps(RecurrentAutoEncoderMixin.init_params)
    def __init__(
        self,         
        input_size: int, hidden_size: int, num_layers: int = 1, bias: bool = True,
        batch_first: bool = True, dropout: float = 0.2, bidirectional: bool = False,
        proj_size: int = 0, nonlinearity: NonLinearity = NonLinearity.Tanh, 
        device: DeviceQ = None, dtype: DTypeQ = None, 
        kind: RecurrentLayer = RecurrentLayer.LSTM, 
        init: InitMethod = InitMethod.orthogonal,
        gain: float = np.sqrt(2),

        make_relu: bool = False,
        make_linear: bool = False,
        linear_size: IntQ = None,
        logger: logging.Logger = None,
    ):
        '''
        Initialize the Encoder component of a recurrent autoencoder.

        Sets up the encoder with the specified configuration, including the recurrent layer, 
        activation functions, and optional linear layers. The initialization also includes 
        configuring the weights of the recurrent and linear layers.

        Parameters
        ----------
        input_size : int
            The number of expected features in the input `x`.

        hidden_size : int
            The number of features in the hidden state `h`.

        num_layers : int, optional
            Number of recurrent layers. Default is 1.

        bias : bool, optional
            If `False`, then the layer does not use bias weights. Default is `True`.

        batch_first : bool, optional
            If `True`, then the input and output tensors are provided as `(batch, seq, feature)`.
            Default is `True`.

        dropout : float, optional
            If non-zero, introduces a dropout layer on the outputs of each RNN layer except the last layer.
            Default is 0.2.

        bidirectional : bool, optional
            If `True`, becomes a bidirectional RNN. Default is `False`.

        proj_size : int, optional
            If > 0, will add a linear projection layer to the output of the RNN. Default is 0.

        nonlinearity : NonLinearity, optional
            The non-linearity to use. Default is `NonLinearity.Tanh`.

        device : DeviceQ, optional
            Device on which to allocate the layer's tensors. Default is `None`.

        dtype : DTypeQ, optional
            Data type for the layer's tensors. Default is `None`.

        kind : RecurrentLayer, optional
            Type of the recurrent layer (e.g., LSTM, GRU). Default is `RecurrentLayer.LSTM`.

        init : InitMethod, optional
            The method used for weight initialization. Default is `InitMethod.orthogonal`.

        gain : float, optional
            Multiplier for initializing weights. Default is `sqrt(2)`.

        make_relu : bool, optional
            If `True`, adds a ReLU activation layer. Default is `False`.
            
        make_linear : bool, optional
            If `True`, adds a linear transformation layer. Default is `False`.
            
        linear_size : IntQ, optional
            Size of the linear layer, only used if `make_linear` is `True`. Default is `None`.
            
        logger : logging.Logger, optional
            Logger for logging messages. Default is `None`.

        Examples
        --------
        >>> encoder = Encoder(input_size=10, hidden_size=20)
        >>> output = encoder(torch.rand(5, 3, 10))
        '''
        super(Encoder, self).__init__()
        self.init_params(
            input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, bias=bias, 
            batch_first=batch_first, dropout=dropout, bidirectional=bidirectional,
            proj_size=proj_size, nonlinearity=nonlinearity, device=device, dtype=dtype,
            kind = kind, decoder = False, init = init, gain =gain, 
            make_relu=make_relu, make_linear=make_linear, linear_size=linear_size, logger=logger
        )

    def latent(self, x, retlast: bool = False) -> Tensor:
        '''
        Compute the latent representation of the input.

        Parameters
        ----------
        x : Tensor
            Input tensor to the encoder.

        retlast : bool, optional
            Whether to return the last output only, defaults to False.

        Returns
        -------
        Tensor
            The latent representation of the input.

        Examples
        --------
        >>> encoder = Encoder(10, 20)
        >>> latent = encoder.latent(torch.rand(5, 3, 10))
        '''
        self.eval()
        out, states = self.forward(x, retlast=retlast)
        hidden = states if self.is_gru() else states[0]
        latent = hidden.squeeze(0).detach()
        return latent

    def forward(self, x, retlast: bool = False) -> Tensor:
        '''
        Forward pass through the encoder.

        Parameters
        ----------
        x : Tensor
            Input tensor to the encoder.
            
        retlast : bool, optional
            Whether to return the last output only, defaults to False.

        Returns
        -------
        (Tensor, RecurrentStatesQ)
            The output from the encoder and the final states.

        Examples
        --------
        >>> encoder = Encoder(10, 20)
        >>> output, states = encoder(torch.rand(5, 3, 10))
        '''
        # set initial hidden and cell states
        x, (h0, c0) = self.prep_inputs(x)
        # forward propagate rmod
        EncoderLogMessage('post-prep', x.shape, h0.shape, c0.shape)(self.log)
        out, (h0, c0) = self.rmod(x, (h0, c0))
        # out: tensor of shape (batch_size, sequence_len, hidden_size) e.g. (769, 3, 12)
        # (h0, c0) both have [num_layers, batch_size, hidden_size]
        EncoderLogMessage('post-rmod', out.shape, h0.shape, c0.shape)(self.log)
        return init_rn(out, retlast), (h0, c0)

# %% ../nbs/40_rcur.ipynb 27
class Decoder(nn.Module, RecurrentAutoEncoderMixin):
    '''
    Decoder component of a recurrent autoencoder.

    Inherits from nn.Module and RecurrentAutoEncoderMixin, and provides methods specific
    to the decoder component of an autoencoder.

    Methods
    -------
    __init__(...):
        Initialize the decoder with specified parameters.
        
    iterdec(out, **kwargs):
        Iteratively decode over time steps.

    prep_states(x, h0, c0):
        Prepare the initial states for the decoder.

    fulldec(out, **kwargs):
        Full decode in one step.

    forward(x, h0, c0, seqlen, retlast):
        Forward pass through the decoder.

    Examples
    --------
    >>> decoder = Decoder(hidden_size=20, output_size=10)
    >>> output = decoder(torch.rand(5, 3, 20))
    '''

    @wraps(RecurrentAutoEncoderMixin.init_params)
    def __init__(
        self, 
        hidden_size: int, output_size: int, num_layers: int = 1, bias: bool = True,
        batch_first: bool = True, dropout: float = 0.2, bidirectional: bool = False,
        proj_size: int = 0, nonlinearity: NonLinearity = NonLinearity.Tanh, 
        device: DeviceQ = None, dtype: DTypeQ = None, 
        kind: RecurrentLayer = RecurrentLayer.LSTM, 
        init: InitMethod = InitMethod.orthogonal,
        gain: float = np.sqrt(2),

        make_relu: bool = False,
        make_linear: bool = True,
        linear_size: IntQ = None,        
        use_encoded_space: bool = True, logger: logging.Logger = None,
    ):
        '''
        Initialize the Decoder component of a recurrent autoencoder.

        Sets up the decoder with the specified configuration, including the recurrent layer, 
        activation functions, and optional linear layers. The initialization also includes 
        configuring the weights of the recurrent and linear layers, and setting up the 
        decoding mechanism.
        
        Parameters
        ----------
        input_size : int
            The number of expected features in the input `x`.

        hidden_size : int
            The number of features in the hidden state `h`.

        num_layers : int, optional
            Number of recurrent layers. Default is 1.

        bias : bool, optional
            If `False`, then the layer does not use bias weights. Default is `True`.

        batch_first : bool, optional
            If `True`, then the input and output tensors are provided as `(batch, seq, feature)`.
            Default is `True`.

        dropout : float, optional
            If non-zero, introduces a dropout layer on the outputs of each RNN layer except the last layer.
            Default is 0.2.

        bidirectional : bool, optional
            If `True`, becomes a bidirectional RNN. Default is `False`.

        proj_size : int, optional
            If > 0, will add a linear projection layer to the output of the RNN. Default is 0.

        nonlinearity : NonLinearity, optional
            The non-linearity to use. Default is `NonLinearity.Tanh`.

        device : DeviceQ, optional
            Device on which to allocate the layer's tensors. Default is `None`.

        dtype : DTypeQ, optional
            Data type for the layer's tensors. Default is `None`.

        kind : RecurrentLayer, optional
            Type of the recurrent layer (e.g., LSTM, GRU). Default is `RecurrentLayer.LSTM`.

        init : InitMethod, optional
            The method used for weight initialization. Default is `InitMethod.orthogonal`.

        gain : float, optional
            Multiplier for initializing weights. Default is `sqrt(2)`.

        make_relu : bool, optional
            If `True`, adds a ReLU activation layer. Default is `False`.
            
        make_linear : bool, optional
            If `True`, adds a linear transformation layer. Default is `False`.
            
        linear_size : IntQ, optional
            Size of the linear layer, only used if `make_linear` is `True`. Default is `None`.
            
        use_encoded_space : bool, optional
            If `True`, the decoder uses the encoded space dimensions for output. Default is `True`.
            
        logger : logging.Logger, optional
            Logger for logging messages. Default is `None`.
        

        Examples
        --------
        >>> decoder = Decoder(hidden_size=20, output_size=10)
        >>> output = decoder(torch.rand(5, 3, 20))
        '''
        super(Decoder, self).__init__()
        params = dict(
            input_size = hidden_size, hidden_size = output_size, num_layers = num_layers, bias = bias, 
            batch_first = batch_first, dropout = dropout, bidirectional = bidirectional, 
            proj_size = proj_size, nonlinearity = nonlinearity, device = device, 
            dtype = dtype, 
            kind = kind, decoder = True, init = init, gain = gain,
            make_relu = make_relu, make_linear = True, linear_size = linear_size, logger = logger
        )
        if use_encoded_space: params.update(hidden_size = hidden_size, linear_size = output_size)
        self.init_params(**params)
        self.use_encoded_space = use_encoded_space        
        
    def iterdec(self, out, **kwargs):
        '''
        Iteratively decode over time steps.

        Parameters
        ----------
        out : Tensor
            Output from the previous layer or step.

        kwargs : Various
            Additional arguments for the decoding process.

        Returns
        -------
        (Tensor, RecurrentStatesQ)
            The output from the decoder and the final states.

        Examples
        --------
        >>> decoder = Decoder(20, 10)
        >>> output, states = decoder.iterdec(torch.rand(5, 3, 20), ...)
        '''
        res, h0, c0, seqlen, retlast = [], *deconstruct(kwargs, 'h0', 'c0', 'seqlen', 'retlast')

        for i in range(seqlen or 1):
            DecoderLogMessage('iter-step', out.shape, h0.shape, c0.shape, step_idx=i)(self.log)
            out, (h0, c0) = self.rmod(out, (h0, c0))
            res.append(out)

        res = torch.cat(res, dim=0)
        if self.use_encoded_space: res = self.full(res)
        DecoderLogMessage('post-iter', res.shape, h0.shape, c0.shape)(self.log)
        return init_rn(res, retlast), (h0, c0)
    
    def prep_states(self, x, h0 = None, c0 = None):
        '''Prepare the initial states for the decoder.

        Parameters
        ----------
        x : Tensor
            Input tensor to the decoder.

        h0 : TensorQ, optional
            Initial hidden state, defaults to None.

        c0 : TensorQ, optional
            Initial cell state, defaults to None.

        Returns
        -------
        (Tensor, RecurrentStatesQ)
            Prepared input tensor and tuple of hidden and cell states.

        Examples
        --------
        >>> decoder = Decoder(20, 10)
        >>> x, states = decoder.prep_states(torch.rand(5, 3, 20))
        '''
        if not self.use_encoded_space:
            if notnone(h0): h0 = self.full(h0) # map from encoder hidden size to decoder hidden size
            if notnone(c0): c0 = self.full(c0) # e.g. (nlayers, hidsize) --> (nlayers, outsize)
        
        out, (h0, c0) = self.prep_inputs(x, h0, c0, hcell=self.output_size, hsize=self.output_size)
        DecoderLogMessage('post-prep', out.shape, h0.shape, c0.shape)(self.log)
        return out, (h0, c0)
        
    def fulldec(self, out, **kwargs):
        '''
        Fully decode the input in one step.

        Parameters
        ----------
        out : Tensor
            Output from the previous layer or step.

        kwargs : Various
            Additional arguments for the decoding process.

        Returns
        -------
        (Tensor, RecurrentStatesQ)
            The output from the decoder and the final states.

        Examples
        --------
        >>> decoder = Decoder(20, 10)
        >>> output, states = decoder.fulldec(torch.rand(5, 3, 20), ...)
        '''
        h0, c0, retlast = deconstruct(kwargs, 'h0', 'c0', 'retlast')
        out, (h0, c0) = self.rmod(out, (h0, c0))
        DecoderLogMessage('post-rmod', out.shape, h0.shape, c0.shape)(self.log)
        return init_rn(out, retlast), (h0, c0)

    def forward(self, x, h0 = None, c0 = None, seqlen: int = None, retlast: bool = False):
        '''
        Forward pass through the decoder.

        Parameters
        ----------
        x : Tensor
            Input tensor to the decoder.

        h0 : TensorQ, optional
            Initial hidden state, defaults to None.

        c0 : TensorQ, optional
            Initial cell state, defaults to None.

        seqlen : int, optional
            Sequence length for decoding, defaults to None.

        retlast : bool, optional
            Whether to return the last output only, defaults to False.

        Returns
        -------
        (Tensor, RecurrentStatesQ)
            The output from the decoder and the final states.

        Examples
        --------
        >>> decoder = Decoder(20, 10)
        >>> output, states = decoder(torch.rand(5, 3, 20))
        '''
        # set initial hidden and cell states
        if not isint(seqlen): seqlen = None
        out, (h0, c0) = self.prep_states(x, h0, c0)        
        if not self.use_encoded_space and seqlen is None:
            return self.fulldec(out, h0=h0, c0=c0, retlast=retlast)
        return self.iterdec(out, h0=h0, c0=c0, seqlen=seqlen, retlast=retlast)

# %% ../nbs/40_rcur.ipynb 29
class AutoEncoder(nn.Module, RecurrentAutoEncoderMixin):
    def __init__(
        self, 
        input_size: int, hidden_size: int, num_layers: int = 1, bias: bool = True,
        batch_first: bool = True, dropout: float = 0.2, bidirectional: bool = False,
        proj_size: int = 0, nonlinearity: NonLinearity = NonLinearity.Tanh, 
        device: DeviceQ = None, dtype: DTypeQ = None, 
        kind: RecurrentLayer = RecurrentLayer.LSTM, 
        init: InitMethod = InitMethod.orthogonal,
        gain: float = np.sqrt(2),

        make_relu: bool = False,
        make_linear: bool = False,
        linear_size: IntQ = None,        
        use_encoded_space: bool = True, logger: logging.Logger = None,
    ):
        '''
        Initialize an AutoEncoder comprising of an encoder and a decoder.

        Configures both the encoder and the decoder components with the specified parameters. 
        The autoencoder can be used for dimensionality reduction, feature learning, or as a 
        generative model.

        Parameters
        ----------
        input_size : int
            The number of expected features in the input `x`.

        hidden_size : int
            The number of features in the hidden state `h`.

        num_layers : int, optional
            Number of recurrent layers. Default is 1.

        bias : bool, optional
            If `False`, then the layer does not use bias weights. Default is `True`.

        batch_first : bool, optional
            If `True`, then the input and output tensors are provided as `(batch, seq, feature)`.
            Default is `True`.

        dropout : float, optional
            If non-zero, introduces a dropout layer on the outputs of each RNN layer except the last layer.
            Default is 0.2.

        bidirectional : bool, optional
            If `True`, becomes a bidirectional RNN. Default is `False`.

        proj_size : int, optional
            If > 0, will add a linear projection layer to the output of the RNN. Default is 0.

        nonlinearity : NonLinearity, optional
            The non-linearity to use. Default is `NonLinearity.Tanh`.

        device : DeviceQ, optional
            Device on which to allocate the layer's tensors. Default is `None`.

        dtype : DTypeQ, optional
            Data type for the layer's tensors. Default is `None`.

        kind : RecurrentLayer, optional
            Type of the recurrent layer (e.g., LSTM, GRU). Default is `RecurrentLayer.LSTM`.

        init : InitMethod, optional
            The method used for weight initialization. Default is `InitMethod.orthogonal`.

        gain : float, optional
            Multiplier for initializing weights. Default is `sqrt(2)`.

        make_relu : bool, optional
            If `True`, adds a ReLU activation layer. Default is `False`.
            
        make_linear : bool, optional
            If `True`, adds a linear transformation layer. Default is `False`.
            
        linear_size : IntQ, optional
            Size of the linear layer, only used if `make_linear` is `True`. Default is `None`.
            
        use_encoded_space : bool, optional
            If `True`, the decoder uses the encoded space dimensions for output. Default is `True`.
            
        logger : logging.Logger, optional
            Logger for logging messages. Default is `None`.

        Examples
        --------
        >>> autoencoder = AutoEncoder(input_size=10, hidden_size=20)
        >>> output = autoencoder(torch.rand(5, 3, 10))
        '''
        super(AutoEncoder, self).__init__()
        params = dict(
            input_size=input_size, hidden_size=hidden_size, output_size=input_size, 
            num_layers=num_layers, bias=bias, batch_first=batch_first, dropout=dropout, 
            bidirectional=bidirectional, proj_size=proj_size, nonlinearity=nonlinearity,
            device=device, dtype=dtype, 
            
            kind = kind, init = init, gain = gain,
            make_relu=make_relu, make_linear=make_linear, linear_size=linear_size, logger = logger,
            
        )
        for k, v in params.items(): setattr(self, k, v)
        
        drop = ('input_size', 'hidden_size', 'output_size', )
        self.encoder = Encoder(input_size, hidden_size, **dropkeys(params, drop))
        self.decoder = Decoder(hidden_size, input_size, **dropkeys(params, drop), use_encoded_space=use_encoded_space)
                
        self.use_encoded_space = use_encoded_space

    @wraps(Encoder.latent)
    def latent(self, x, retlast: bool = False):
        '''
        Compute the latent representation of the input using the encoder.

        Parameters
        ----------
        x : Tensor
            Input tensor to the autoencoder.
            
        retlast : bool, optional
            Whether to return the last output only, defaults to False.

        Returns
        -------
        Tensor
            The latent representation of the input.

        Examples
        --------
        >>> autoencoder = AutoEncoder(10, 20)
        >>> latent = autoencoder.latent(torch.rand(5, 3, 10))
        '''
        return self.encoder.latent(x, retlast)

    @wraps(Encoder.forward)
    def encode(self, x, retlast: bool = False):
        '''
        Encode the input using the encoder component of the autoencoder.

        Parameters
        ----------
        x : Tensor
            Input tensor to the autoencoder.
            
        retlast : bool, optional
            Whether to return the last output only, defaults to False.

        Returns
        -------
        (Tensor, RecurrentStatesQ)
            The encoded output and the final states.

        Examples
        --------
        >>> autoencoder = AutoEncoder(10, 20)
        >>> encoded, states = autoencoder.encode(torch.rand(5, 3, 10))
        '''
        return self.encoder(x, retlast)
    
    @wraps(Decoder.forward)
    def decode(self, x, h0 = None, c0 = None, seqlen: int = None, retlast: bool = False):
        '''
        Decode the input using the decoder component of the autoencoder.

        Parameters
        ----------
        x : Tensor
            Input tensor to the autoencoder.

        h0 : TensorQ, optional
            Initial hidden state for the decoder, defaults to None.

        c0 : TensorQ, optional
            Initial cell state for the decoder (for LSTM), defaults to None.

        seqlen : int, optional
            Sequence length for decoding, defaults to None.

        retlast : bool, optional
            Whether to return the last output only, defaults to False.

        Returns
        -------
        (Tensor, RecurrentStatesQ)
            The decoded output and the final states.

        Examples
        --------
        >>> autoencoder = AutoEncoder(10, 20)
        >>> decoded, states = autoencoder.decode(torch.rand(5, 3, 10))
        '''
        return self.decoder(x, h0, c0, seqlen=seqlen, retlast=retlast)

    def forward(self, x, retlast: bool = False, width: IntQ = None, height: IntQ = None):
        '''
        Forward pass through the autoencoder, encoding and then decoding the input.

        Parameters
        ----------
        x : Tensor
            Input tensor to the autoencoder.

        retlast : bool, optional
            Whether to return the last output only, defaults to False.

        width : IntQ, optional
            Width dimension for reshaping the output, defaults to None.

        height : IntQ, optional
            Height dimension for reshaping the output, defaults to None.

        Returns
        -------
        Tensor
            The output from the autoencoder after encoding and decoding.

        Examples
        --------
        >>> autoencoder = AutoEncoder(10, 20)
        >>> output = autoencoder(torch.rand(5, 3, 10))
        '''
        # x.shape = [769, 3, 4096]
        seqlen = channels(x, self.batch_first) # 3
        enc_x, (h0, c0) = self.encoder(x, retlast=retlast) # encoded_x.shape = [769, 1, 12]
        seqlen = (seqlen or 1) if self.use_encoded_space else None
        
        dec_x, (h0, c0) = self.decoder(
            enc_x, h0, c0, seqlen=seqlen, retlast=retlast
        ) # decoded_x.shape = [769, nchannels, 4096]
        if notnone(width) and notnone(height):
            dec_x = as4d(dec_x, width, height, seqlen)
        return dec_x
