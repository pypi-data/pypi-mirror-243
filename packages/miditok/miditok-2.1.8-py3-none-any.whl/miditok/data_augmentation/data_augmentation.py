"""Data augmentation methods

"""
import json
import warnings
from copy import deepcopy
from pathlib import Path
from typing import Dict, List, Tuple, Union
from warnings import warn

import numpy as np
from miditoolkit import MidiFile
from tqdm import tqdm


def data_augmentation_dataset(
    data_path: Union[Path, str],
    tokenizer=None,
    nb_octave_offset: int = None,
    nb_vel_offset: int = None,
    nb_dur_offset: int = None,
    octave_directions: Tuple[bool, bool] = (True, True),
    vel_directions: Tuple[bool, bool] = (True, True),
    dur_directions: Tuple[bool, bool] = (True, True),
    all_offset_combinations: bool = False,
    out_path: Union[Path, str] = None,
    copy_original_in_new_location: bool = True,
    save_data_aug_report: bool = True,
):
    r"""Perform data augmentation on a whole dataset, on the pitch dimension.
    Drum tracks are not augmented.
    The new created files have names in two parts, separated with a 'ยง' character.
    Make sure your files do not have 'ยง' in their names if you intend to reuse the information of the
    second part in some script.
    For note duration augmentation, only the duration, i.e. the ending / offset times of the notes are altered,
    while starting / onset times are unchanged.
    **Note: data augmentation on note durations is not implemented at the moment for MIDI files. Duration offsets will
    be skipped. Alternatively, you can perform data augmentation on note duration on JSON token files.**

    :param data_path: root path to the folder containing tokenized json files.
    :param tokenizer: tokenizer, needs to have 'Pitch' or 'NoteOn' tokens. Has to be given
            if performing augmentation on tokens (default: None).
    :param nb_octave_offset: number of pitch octaves offset to perform data augmentation.
    :param nb_vel_offset: number of velocity values
    :param nb_dur_offset: number of pitch octaves offset to perform data augmentation.
    :param octave_directions: directions to shift the pitch augmentation, for up / down
            as a tuple of two booleans. (default: (True, True))
    :param vel_directions: directions to shift the velocity augmentation, for up / down
            as a tuple of two booleans. (default: (True, True))
    :param dur_directions: directions to shift the duration augmentation, for up / down
            as a tuple of two booleans. (default: (True, True))
    :param all_offset_combinations: will perform data augmentation on all the possible
            combinations of offsets. If set to False, will perform data augmentation
            only based on the original sample.
    :param out_path: output path to save the augmented files. Original (non-augmented) MIDIs will be
            saved to this location. If none is given, they will be saved in the same location an the
            data_path. (default: None)
    :param copy_original_in_new_location: if given True, the original (non-augmented) MIDIs will be saved
            in the out_path location too. (default: True)
    :param save_data_aug_report: will save numbers from the data augmentation in a ``data_augmentation_report.txt``
            file in the output directory. (default: True)
    """
    if out_path is None:
        out_path = data_path
    else:
        if isinstance(out_path, str):
            out_path = Path(out_path)
        out_path.mkdir(parents=True, exist_ok=True)
    as_tokens = True
    files_paths = list(Path(data_path).glob("**/*.json"))
    if len(files_paths) == 0:
        files_paths = list(Path(data_path).glob("**/*.mid"))
        as_tokens = False

    nb_augmentations, nb_tracks_augmented = 0, 0
    for file_path in tqdm(files_paths, desc="Performing data augmentation"):
        if as_tokens:
            with open(file_path) as json_file:
                file = json.load(json_file)
                ids = file["ids"]
                programs = file["programs"] if "programs" in file else None

            if tokenizer.one_token_stream:
                ids = [ids]

            # Perform data augmentation for each track
            offsets = get_offsets(
                tokenizer,
                nb_octave_offset,
                nb_vel_offset,
                nb_dur_offset,
                octave_directions,
                vel_directions,
                dur_directions,
                ids=ids,
            )
            augmented_tokens: Dict[
                Tuple[int, int, int], List[Union[int, List[int]]]
            ] = {}
            for ti, track in enumerate(ids):
                # we dont augment drums
                if programs is not None and programs[ti][1]:  # drums
                    continue
                elif (
                    tokenizer.one_token_stream
                    and programs is not None
                    and all(p[1] for p in programs)
                ):
                    continue
                corrected_offsets = deepcopy(offsets)
                vel_dim = int(128 / len(tokenizer.velocities))
                corrected_offsets[1] = [
                    int(off / vel_dim) for off in corrected_offsets[1]
                ]
                aug = data_augmentation_tokens(
                    np.array(track),
                    tokenizer,
                    *corrected_offsets,
                    all_offset_combinations=all_offset_combinations,
                )
                if len(aug) == 0:
                    continue
                for aug_offsets, seq in aug:
                    if tokenizer.one_token_stream:
                        augmented_tokens[aug_offsets] = seq
                        continue
                    try:
                        augmented_tokens[aug_offsets].append(seq)
                    except KeyError:
                        augmented_tokens[aug_offsets] = [seq]
            if not tokenizer.one_token_stream and programs is not None:
                for i, (track, (_, is_drum)) in enumerate(
                    zip(ids, programs)
                ):  # adding drums to all already augmented
                    if is_drum:
                        for aug_offsets in augmented_tokens:
                            augmented_tokens[aug_offsets].insert(i, track)

            # Save augmented tracks as json
            for aug_offsets, tracks_seq in augmented_tokens.items():
                if len(tracks_seq) == 0:
                    continue
                suffix = "ยง" + "_".join(
                    [
                        f"{t}{offset}"
                        for t, offset in zip(["p", "v", "d"], aug_offsets)
                        if offset != 0
                    ]
                )
                saving_path = out_path / file_path.parent.relative_to(data_path)
                saving_path.mkdir(parents=True, exist_ok=True)
                saving_path /= f"{file_path.stem}{suffix}.json"
                tokenizer.save_tokens(tracks_seq, saving_path, programs)
                nb_augmentations += 1
                nb_tracks_augmented += len(tracks_seq)
            if copy_original_in_new_location and out_path != data_path:
                if tokenizer.one_token_stream:
                    ids = ids[0]
                saving_path = (
                    out_path
                    / file_path.parent.relative_to(data_path)
                    / f"{file_path.stem}.json"
                )
                saving_path.parent.mkdir(parents=True, exist_ok=True)
                tokenizer.save_tokens(ids, saving_path, programs)

        else:  # as midi
            try:
                midi = MidiFile(file_path)
            except Exception:
                # ValueError, OSError, FileNotFoundError, IOError, EOFError, mido.KeySignatureError
                continue

            offsets = get_offsets(
                tokenizer,
                nb_octave_offset,
                nb_vel_offset,
                nb_dur_offset,
                octave_directions,
                vel_directions,
                dur_directions,
                midi=midi,
            )
            augmented_midis = data_augmentation_midi(
                midi,
                tokenizer,
                *offsets,
                all_offset_combinations=all_offset_combinations,
            )
            for aug_offsets, aug_midi in augmented_midis:
                if len(aug_midi.instruments) == 0:
                    continue
                suffix = "ยง" + "_".join(
                    [
                        f"{t}{offset}"
                        for t, offset in zip(["p", "v", "d"], aug_offsets)
                        if offset != 0
                    ]
                )
                saving_path = out_path / file_path.parent.relative_to(data_path)
                saving_path.mkdir(parents=True, exist_ok=True)
                saving_path /= f"{file_path.stem}{suffix}.mid"
                aug_midi.dump(saving_path)
                nb_augmentations += 1
                nb_tracks_augmented += len(aug_midi.instruments)
            if copy_original_in_new_location and out_path != data_path:
                saving_path = (
                    out_path
                    / file_path.parent.relative_to(data_path)
                    / f"{file_path.stem}.mid"
                )
                saving_path.parent.mkdir(parents=True, exist_ok=True)
                midi.dump(saving_path)

    # Saves data augmentation report, json encoded with txt extension to not mess with others json files
    if save_data_aug_report:
        with open(out_path / "data_augmentation_report.txt", "w") as outfile:
            json.dump(
                {
                    "nb_tracks_augmented": nb_tracks_augmented,
                    "nb_files_before": len(files_paths),
                    "nb_files_after": len(files_paths) + nb_augmentations,
                },
                outfile,
            )


def get_offsets(
    tokenizer=None,
    nb_octave_offset: int = None,
    nb_vel_offset: int = None,
    nb_dur_offset: int = None,
    octave_directions: Tuple[bool, bool] = (True, True),
    vel_directions: Tuple[bool, bool] = (True, True),
    dur_directions: Tuple[bool, bool] = (True, True),
    midi: MidiFile = None,
    ids: List[Union[int, List[int]]] = None,
) -> List[List[int]]:
    r"""Build the offsets in absolute value for data augmentation.
    TODO some sort of limit for velocity and duration values (min / max as for octaves)

    :param tokenizer: tokenizer, needs to have 'Pitch' tokens.
    :param nb_octave_offset: number of pitch octaves offset to perform data augmentation.
    :param nb_vel_offset: number of velocity values
    :param nb_dur_offset: number of pitch octaves offset to perform data augmentation.
    :param octave_directions: directions to shift the pitch augmentation, for up / down
            as a tuple of two booleans. (default: (True, True))
    :param vel_directions: directions to shift the velocity augmentation, for up / down
            as a tuple of two booleans. (default: (True, True))
    :param dur_directions: directions to shift the duration augmentation, for up / down
            as a tuple of two booleans. (default: (True, True))
    :param midi: midi object to augment (default: None)
    :param ids: token ids as a list of tracks (default: None)
    :return: the offsets of pitch, velocity and duration features, in "absolute" value
    """
    offsets = []

    if nb_octave_offset is not None:
        # Get the maximum and lowest pitch in original track
        all_pitches = []
        if midi is not None:
            for track in midi.instruments:
                if not track.is_drum:
                    all_pitches += [note.pitch for note in track.notes]
            max_pitch, min_pitch = max(all_pitches), min(all_pitches)
        else:
            pitch_voc_idx = (
                tokenizer.vocab_types_idx["Pitch"] if tokenizer.is_multi_voc else None
            )
            ids_pitch = []
            if not tokenizer.is_multi_voc:
                pitch_ids_vocab = np.array(
                    tokenizer.token_ids_of_type("Pitch")
                    + tokenizer.token_ids_of_type("NoteOn")
                )
                for track_ids in ids:
                    tt_arr = np.array(track_ids)
                    ids_pitch.append(tt_arr[np.isin(tt_arr, pitch_ids_vocab)])
                max_pitch = tokenizer[int(np.max(np.concatenate(ids_pitch)))].split(
                    "_"
                )[1]
                min_pitch = tokenizer[int(np.min(np.concatenate(ids_pitch)))].split(
                    "_"
                )[1]
            else:
                pitch_ids_vocab = np.array(
                    tokenizer.token_ids_of_type("Pitch", pitch_voc_idx)
                )
                for track_ids in ids:
                    tt_arr = np.array(track_ids)[:, pitch_voc_idx]
                    ids_pitch.append(tt_arr[np.isin(tt_arr, pitch_ids_vocab)])
                max_pitch = tokenizer[
                    pitch_voc_idx, int(np.max(np.concatenate(ids_pitch)))
                ].split("_")[1]
                min_pitch = tokenizer[
                    pitch_voc_idx, int(np.min(np.concatenate(ids_pitch)))
                ].split("_")[1]
        offset_up = min(
            nb_octave_offset,
            (tokenizer.config.pitch_range[1] - 1 - int(max_pitch)) // 12,
        )
        offset_down = min(
            nb_octave_offset, (int(min_pitch) - tokenizer.config.pitch_range[0]) // 12
        )

        off = []
        if octave_directions[0]:
            off += list(range(12, offset_up * 12 + 1, 12))
        if octave_directions[1]:
            off += list(range(-offset_down * 12, 0, 12))
        offsets.append(off)

    if nb_vel_offset is not None:
        vel_dim = int(128 / len(tokenizer.velocities))
        off = []
        if vel_directions[0]:
            off += list(range(vel_dim, nb_vel_offset * vel_dim + 1, vel_dim))
        if vel_directions[1]:
            off += list(range(-nb_vel_offset * vel_dim, 0, vel_dim))
        offsets.append(off)

    if nb_dur_offset is not None:
        off = []
        if dur_directions[0]:
            off += list(range(1, nb_dur_offset + 1))
        if dur_directions[1]:
            off += list(range(-nb_dur_offset, 0))
        offsets.append(off)

    return offsets


def data_augmentation_midi(
    midi: MidiFile,
    tokenizer,
    pitch_offsets: List[int] = None,
    velocity_offsets: List[int] = None,
    duration_offsets: List[int] = None,
    all_offset_combinations: bool = False,
) -> List[Tuple[Tuple[int, int, int], MidiFile]]:
    r"""Perform data augmentation on a MIDI object.
    Drum tracks are not augmented, but copied as original in augmented MIDIs.
    **Note: data augmentation on note durations is not implemented at the moment for MIDI files. Duration offsets will
    be skipped. Alternatively, you can perform data augmentation on note duration on JSON token files.**

    :param midi: midi object to augment
    :param tokenizer: tokenizer, needs to have 'Pitch' tokens.
    :param pitch_offsets: list of pitch offsets for augmentation.
    :param velocity_offsets: list of velocity offsets for augmentation.
    :param duration_offsets: list of duration offsets for augmentation.
    :param all_offset_combinations: will perform data augmentation on all the possible
            combinations of offsets. If set to False, will perform data augmentation
            only based on the original sample.
    :return: augmented MIDI objects.
    """
    augmented = []

    # Pitch augmentation
    if pitch_offsets is not None:
        for offset in pitch_offsets:
            midi_augmented = deepcopy(midi)
            for track in midi_augmented.instruments:
                if not track.is_drum:
                    for note in track.notes:
                        note.pitch += offset
            augmented.append(((offset, 0, 0), midi_augmented))

    # Velocity augmentation
    if velocity_offsets is not None:

        def augment_vel(
            midi_: MidiFile, offsets_: Tuple[int, int, int]
        ) -> List[Tuple[Tuple[int, int, int], MidiFile]]:
            aug_ = []
            for offset_ in velocity_offsets:
                midi_aug_ = deepcopy(midi_)
                for track_ in midi_aug_.instruments:
                    for note_ in track_.notes:
                        if offset_ < 0:
                            note_.velocity = max(
                                min(tokenizer.velocities), note_.velocity + offset_
                            )
                        else:
                            note_.velocity = min(
                                max(tokenizer.velocities), note_.velocity + offset_
                            )
                aug_.append(((offsets_[0], offset_, offsets_[2]), midi_aug_))
            return aug_

        if all_offset_combinations:
            for i in range(len(augmented)):
                offsets, midi_aug = augmented[i]
                augmented += augment_vel(
                    midi_aug, offsets
                )  # for already augmented midis
        augmented += augment_vel(midi, (0, 0, 0))  # for original midi

    # Duration augmentation
    if duration_offsets is not None:
        warnings.warn(
            "You provided duration offsets for data augmentation on MIDI files. Data augmentation on note"
            "durations is not implemented in MidiTok at the moment. Your duration offsets will be skipped."
            "Alternatively, you can perform data augmentation on note duration on JSON token files."
        )
        # TODO implement it, and remove warning above + update doc
        """tokenizer.durations_ticks[midi.ticks_per_beat] = np.array([(beat * res + pos) * midi.ticks_per_beat // res
                                                                           for beat, pos, res in tokenizer.durations])
        def augment_dur(midi_: MidiFile, offsets_: Tuple[int, int, int]) -> List[Tuple[Tuple[int, int, int], MidiFile]]:
            aug_ = []
            dur_bins = tokenizer.durations_ticks[tokenizer.current_midi_metadata['time_division']]

            for offset_ in dur_offsets:
                midi_aug_ = deepcopy(midi_)
                for track_ in midi_aug_.instruments:
                    for note_ in track_.notes:
                        duration = note.end - note.start
                        index = np.argmin(np.abs(dur_bins - duration))

                        note_.end += offset_  # TODO multiply token val
                aug_.append(((offsets_[0], offsets_[1], offset_), midi_aug_))
            return aug_

        if all_offset_combinations:
            for i in range(len(augmented)):
                offsets, midi_aug = augmented[i]
                augmented += augment_dur(midi_aug, offsets)  # for already augmented midis
        augmented += augment_dur(midi, (0, 0, 0))  # for original midi"""

    return augmented


def data_augmentation_tokens(
    tokens: Union[np.ndarray, List[int]],
    tokenizer,
    pitch_offsets: List[int] = None,
    velocity_offsets: List[int] = None,
    duration_offsets: List[int] = None,
    all_offset_combinations: bool = False,
) -> List[Tuple[Tuple[int, int, int], List[int]]]:
    r"""Perform data augmentation on a sequence of tokens, on the pitch dimension.
    NOTE: token sequences with BPE will be decoded during the augmentation, this might take some time.
    NOTE 2: the tokenizer must have a vocabulary in which the pitch values increase with the token index,
    e.g. Pitch_48: token 64, Pitch_49: token65 ...
    The same goes for durations.
    MIDILike is not compatible with data augmentation on durations.
    MuMIDI is not compatible at all.

    :param tokens: tokens to perform data augmentation on.
    :param tokenizer: tokenizer, needs to have 'Pitch' tokens.
    :param pitch_offsets: list of pitch offsets for augmentation.
    :param velocity_offsets: list of velocity offsets for augmentation.
    :param duration_offsets: list of duration offsets for augmentation.
    :param all_offset_combinations: will perform data augmentation on all the possible
            combinations of offsets. If set to False, will perform data augmentation
            only based on the original sample.
    :return: the several data augmentations that have been performed
    """
    augmented = []

    # Decode BPE
    bpe_decoded = False
    if tokenizer.has_bpe:
        tokens = tokenizer.decode_bpe(
            tokens.tolist() if isinstance(tokens, np.ndarray) else tokens
        )
        bpe_decoded = True

    # Converts to np array if necessary
    if not isinstance(tokens, np.ndarray):
        tokens = np.array(tokens)

    if pitch_offsets is not None:
        # Get the maximum and lowest pitch in original track
        pitch_voc_idx = (
            tokenizer.vocab_types_idx["Pitch"] if tokenizer.is_multi_voc else None
        )
        note_off_tokens = []
        if not tokenizer.is_multi_voc:
            pitch_tokens = np.array(
                tokenizer.token_ids_of_type("Pitch")
                + tokenizer.token_ids_of_type("NoteOn")
            )
            note_off_tokens = np.array(tokenizer.token_ids_of_type("NoteOff"))
            mask_pitch = np.isin(tokens, pitch_tokens)
            # If applicable, removes drum notes from the mask
            if tokenizer.one_token_stream:
                for idx, is_note in enumerate(mask_pitch):
                    if (
                        is_note
                        and idx > 0
                        and tokenizer[tokens[idx - 1]] == "Program_-1"
                    ):
                        mask_pitch[idx] = False
                        if len(note_off_tokens) > 0:
                            note_off_tokens[idx] = False
        else:
            pitch_tokens = np.array(tokenizer.token_ids_of_type("Pitch", pitch_voc_idx))
            mask_pitch = np.full_like(tokens, 0, dtype=np.bool_)
            mask_pitch[:, pitch_voc_idx] = np.isin(
                tokens[:, pitch_voc_idx], pitch_tokens
            )

        # Perform augmentation on pitch
        for offset in pitch_offsets:
            seq = tokens.copy()
            seq[mask_pitch] += offset
            if len(note_off_tokens) > 0:  # for MIDIlike
                seq[np.isin(seq, note_off_tokens)] += offset
            augmented.append(((offset, 0, 0), seq))

    # Velocity augmentation
    if velocity_offsets is not None:
        vel_voc_idx = (
            tokenizer.vocab_types_idx["Velocity"] if tokenizer.is_multi_voc else None
        )
        if not tokenizer.is_multi_voc:
            vel_tokens = np.array(tokenizer.token_ids_of_type("Velocity"))
        else:
            vel_tokens = np.array(tokenizer.token_ids_of_type("Velocity", vel_voc_idx))

        def augment_vel(
            seq_: np.ndarray, offsets_: Tuple[int, int, int]
        ) -> List[Tuple[Tuple[int, int, int], np.ndarray]]:
            if not tokenizer.is_multi_voc:
                mask = np.isin(seq_, vel_tokens)
            else:
                mask = np.full_like(seq_, 0, dtype=np.bool_)
                mask[:, vel_voc_idx] = np.isin(seq_[:, vel_voc_idx], vel_tokens)

            aug_ = []
            for offset_ in velocity_offsets:
                aug_seq = seq_.copy()
                aug_seq[mask] += offset_
                aug_seq[mask] = np.clip(aug_seq[mask], vel_tokens[0], vel_tokens[-1])
                aug_.append(((offsets_[0], offset_, offsets_[2]), aug_seq))
            return aug_

        if all_offset_combinations:
            for i in range(len(augmented)):
                offsets, seq_aug = augmented[i]
                augmented += augment_vel(
                    seq_aug, offsets
                )  # for already augmented midis
        augmented += augment_vel(tokens, (0, 0, 0))  # for original midi

    # Duration augmentation
    if duration_offsets is not None and type(tokenizer).__name__ == "MIDILike":
        warn(
            f"{type(tokenizer).__name__} is not compatible with data augmentation on duration at token level"
        )
    elif duration_offsets is not None:
        dur_voc_idx = (
            tokenizer.vocab_types_idx["Duration"] if tokenizer.is_multi_voc else None
        )
        if not tokenizer.is_multi_voc:
            dur_tokens = np.array(tokenizer.token_ids_of_type("Duration"))
        else:
            dur_tokens = np.array(tokenizer.token_ids_of_type("Duration", dur_voc_idx))

        def augment_dur(
            seq_: np.ndarray, offsets_: Tuple[int, int, int]
        ) -> List[Tuple[Tuple[int, int, int], np.ndarray]]:
            if not tokenizer.is_multi_voc:
                mask = np.isin(seq_, dur_tokens)
            else:
                mask = np.full_like(seq_, 0, dtype=np.bool_)
                mask[:, dur_voc_idx] = np.isin(seq_[:, dur_voc_idx], dur_tokens)

            aug_ = []
            for offset_ in duration_offsets:
                aug_seq = seq_.copy()
                aug_seq[mask] += offset_
                aug_seq[mask] = np.clip(aug_seq[mask], dur_tokens[0], dur_tokens[-1])
                aug_.append(((offsets_[0], offsets_[1], offset_), aug_seq))
            return aug_

        if all_offset_combinations:
            for i in range(len(augmented)):
                offsets, seq_aug = augmented[i]
                augmented += augment_dur(
                    seq_aug, offsets
                )  # for already augmented midis
        augmented += augment_dur(tokens, (0, 0, 0))  # for original midi

    # Convert all arrays to lists and reapply BPE if necessary
    for i in range(len(augmented)):
        augmented[i] = (
            augmented[i][0],
            tokenizer.apply_bpe(augmented[i][1].tolist())
            if bpe_decoded
            else augmented[i][1].tolist(),
        )

    return augmented
