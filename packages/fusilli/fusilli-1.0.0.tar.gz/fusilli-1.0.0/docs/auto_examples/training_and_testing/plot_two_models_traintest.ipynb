{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Regression: Comparing Two Tabular Models Trained on Simulated Data\n\n\ud83d\ude80 Welcome to this tutorial on training and comparing two fusion models on a regression task using simulated multimodal tabular data! \ud83c\udf89\n\n\ud83c\udf1f Key Features:\n\n- \ud83d\udce5 Importing models based on name.\n- \ud83e\uddea Training and testing models with train/test protocol.\n- \ud83d\udcbe Saving trained models to a dictionary for later analysis.\n- \ud83d\udcca Plotting the results of a single model.\n- \ud83d\udcc8 Plotting the results of multiple models as a bar chart.\n- \ud83d\udcbe Saving the results of multiple models as a CSV file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import importlib\n\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nimport os\n\nfrom docs.examples import generate_sklearn_simulated_data\nfrom fusilli.data import get_data_module\nfrom fusilli.eval import RealsVsPreds, ModelComparison\nfrom fusilli.train import train_and_save_models\nfrom fusilli.utils.model_chooser import import_chosen_fusion_models\n\n# sphinx_gallery_thumbnail_number = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import fusion models \ud83d\udd0d\nLet's kick things off by importing our fusion models. The models are imported using the\n:func:`~fusilli.utils.model_chooser.import_chosen_fusion_models` function, which takes a dictionary of conditions\nas an input. The conditions are the attributes of the models, e.g. the class name, the modality type, etc.\n\nThe function returns list of class objects that match the conditions. If no conditions are specified, then all the models are returned.\n\nWe're importing ConcatTabularData and TabularChannelWiseMultiAttention models for this example. Both are multimodal tabular models.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_conditions = {\n    \"class_name\": [\"ConcatTabularData\", \"TabularChannelWiseMultiAttention\"],\n}\n\nfusion_models = import_chosen_fusion_models(model_conditions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Set the training parameters \ud83c\udfaf\nNow, let's configure our training parameters. The parameters are stored in a dictionary and passed to most\nof the methods in this library.\nFor training and testing, the necessary parameters are:\n\n- ``test_size``: the proportion of the data to be used for testing.\n- ``kfold_flag``: the user sets this to False for train/test protocol.\n- ``log``: a boolean of whether to log the results using Weights and Biases (True) or not (False).\n- ``pred_type``: the type of prediction to be performed. This is either ``regression``, ``binary``, or ``classification``. For this example we're using regression.\n- ``loss_log_dir``: the directory to save the loss logs to. This is used for plotting the loss curves.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "params = {\n    \"test_size\": 0.2,\n    \"kfold_flag\": False,\n    \"log\": False,\n    \"pred_type\": \"regression\",\n    \"loss_log_dir\": \"loss_logs/two_models_traintest\",  # where the csv of the loss is saved for plotting later\n}\n\n# empty the loss log directory\nfor dir in os.listdir(params[\"loss_log_dir\"]):\n    for file in os.listdir(os.path.join(params[\"loss_log_dir\"], dir)):\n        os.remove(os.path.join(params[\"loss_log_dir\"], dir, file))\n    # remove dir\n    os.rmdir(os.path.join(params[\"loss_log_dir\"], dir))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Generating simulated data \ud83d\udd2e\nTime to create some simulated data for our models to work their wonders on.\nThis function also simulated image data which we aren't using here.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "params = generate_sklearn_simulated_data(\n    num_samples=500,\n    num_tab1_features=10,\n    num_tab2_features=10,\n    img_dims=(1, 100, 100),\n    params=params,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Training the first fusion model \ud83c\udfc1\nHere we train the first fusion model. We're using the ``train_and_save_models`` function to train and test the models.\nThis function takes the following inputs:\n\n- ``trained_models_dict``: a dictionary to store the trained models.\n- ``data_module``: the data module containing the data.\n- ``params``: the parameters for training and testing.\n- ``fusion_model``: the fusion model to be trained.\n\nFirst we'll create a dictionary to store both the trained models so we can compare them later.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "all_trained_models = {}  # create dictionary to store trained models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To train the first model we need to:\n\n1. *Choose the model*: We're using the first model in the ``fusion_models`` list we made earlier.\n2. *Print the attributes of the model*: To check it's been initialised correctly.\n3. *Create the datamodule*: This is done with the :func:`~fusilli.data.get_data_module` function. This function takes the initialised model and the parameters as inputs. It returns the datamodule.\n4. *Train and test the model*: This is done with the :func:`~fusilli.train.train_and_save_models` function. This function takes the datamodule, the parameters, the fusion model, and the initialised model as inputs. It returns a list of the trained models (in this case, only one model).\n5. *Add the trained model to the ``all_trained_models`` dictionary*: This is so we can compare the results of the two models later.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fusion_model = fusion_models[0]\n\nprint(\"Method name:\", fusion_model.method_name)\nprint(\"Modality type:\", fusion_model.modality_type)\nprint(\"Fusion type:\", fusion_model.fusion_type)\n\n# Create the data module\ndm = get_data_module(fusion_model=fusion_model, params=params)\n\n# Train and test\nmodel_1_list = train_and_save_models(\n    data_module=dm,\n    params=params,\n    fusion_model=fusion_model,\n    enable_checkpointing=False,  # False for the example notebooks\n    show_loss_plot=True,\n)\n\n# Add trained model to dictionary\nall_trained_models[fusion_model.__name__] = model_1_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Plotting the results of the first model \ud83d\udcca\nLet's unveil the results of our first model's hard work. We're using the :class:`~fusilli.eval.RealsVsPreds` class to plot the results of the first model.\nThis class takes the trained model as an input and returns a plot of the real values vs the predicted values from the final validation data (when using from_final_val_data).\nIf you want to plot the results from the test data, you can use from_new_data instead. See the example notebook on plotting with new data for more detail.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "reals_preds_model_1 = RealsVsPreds.from_final_val_data(model_1_list)\n\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training the second fusion model \ud83c\udfc1\n It's time for our second fusion model to shine! Here we train the second fusion model: TabularChannelWiseMultiAttention. We're using the same steps as before, but this time we're using the second model in the ``fusion_models`` list.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Choose the model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fusion_model = fusion_models[1]\n\nprint(\"Method name:\", fusion_model.method_name)\nprint(\"Modality type:\", fusion_model.modality_type)\nprint(\"Fusion type:\", fusion_model.fusion_type)\n\n# Create the data module\ndm = get_data_module(fusion_model=fusion_model, params=params)\n\n# Train and test\nmodel_2_list = train_and_save_models(\n    data_module=dm,\n    params=params,\n    fusion_model=fusion_model,\n    enable_checkpointing=False,  # False for the example notebooks\n    show_loss_plot=True,\n)\n\n# Add trained model to dictionary\nall_trained_models[fusion_model.__name__] = model_2_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Plotting the results of the second model \ud83d\udcca\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "reals_preds_model_2 = RealsVsPreds.from_final_val_data(model_2_list)\n\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Comparing the results of the two models \ud83d\udcc8\nLet the ultimate showdown begin! We're comparing the results of our two models.\nWe're using the :class:`~fusilli.eval.ModelComparison` class to compare the results of the two models.\nThis class takes the trained models as an input and returns a plot of the results of the two models and a Pandas DataFrame of the metrics of the two models.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "comparison_plot, metrics_dataframe = ModelComparison.from_final_val_data(\n    all_trained_models\n)\n\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Saving the metrics of the two models \ud83d\udcbe\nTime to archive our models' achievements. We're using the :class:`~fusilli.eval.ModelComparison` class to save the metrics of the two models.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "metrics_dataframe"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}