
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/training_and_testing/plot_model_comparison_loop_kfold.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_training_and_testing_plot_model_comparison_loop_kfold.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_training_and_testing_plot_model_comparison_loop_kfold.py:


Training multiple models in a loop: k-fold regression
====================================================================

Welcome to the "Comparing Multiple K-Fold Trained Fusion Models" tutorial! In this tutorial, we'll explore how to train and compare multiple fusion models for a regression task using k-fold cross-validation with multimodal tabular data. This tutorial is designed to help you understand and implement key features, including:

- üì• Importing fusion models based on modality types.
- üö≤ Setting training parameters for your models
- üîÆ Generating simulated data for experimentation.
- üß™ Training and evaluating multiple fusion models.
- üìà Visualising the results of individual models.
- üìä Comparing the performance of different models.

Let's dive into each of these steps in detail:

1. **Importing Fusion Models:**

   We begin by importing fusion models based on modality types. These models will be used in our regression task, and we'll explore various fusion strategies.

2. **Setting the Training Parameters:**

   To ensure consistent and controlled training, we define training parameters. These parameters include enabling k-fold cross-validation, specifying the prediction type (regression), and setting the batch size for training.

3. **Generating Simulated Data:**

   In this step, we generate synthetic data to simulate a real-world multimodal dataset. This dataset includes two tabular modalities, but it can also incorporate image data, although we won't use images in this example.

4. **Training All Fusion Models:**

   Now, we train all the selected fusion models using the generated data and the defined training parameters. We'll monitor the performance of each model during training and store the results for later analysis.

5. **Plotting Individual Model Results:**

   After training, we visualise the performance of each individual model. We create plots that show loss curves and performance metrics to help us understand how each model performed.

6. **Comparing Model Performance:**

   To gain insights into which fusion models perform best, we compare their performance using a violin chart. This chart provides a clear overview of how each model's performance metrics compare.

7. **Saving the Results:**

   Finally, we save the performance results of all the models as a structured DataFrame. This data can be further analyzed, exported to a CSV file, or used for additional experiments.

Now, let's walk through each of these steps in code and detail. Let's get started! üå∏

.. GENERATED FROM PYTHON SOURCE LINES 46-59

.. code-block:: Python


    import matplotlib.pyplot as plt
    from tqdm.auto import tqdm
    import os

    from docs.examples import generate_sklearn_simulated_data
    from fusilli.data import get_data_module
    from fusilli.eval import RealsVsPreds, ModelComparison
    from fusilli.train import train_and_save_models
    from fusilli.utils.model_chooser import import_chosen_fusion_models

    # from IPython.utils import io  # for hiding the tqdm progress bar








.. GENERATED FROM PYTHON SOURCE LINES 60-71

1. Import fusion models üîç
---------------------------
Here we import the fusion models to be compared. The models are imported using the
:func:`~fusilli.utils.model_chooser.get_models` function, which takes a dictionary of conditions
as an input. The conditions are the attributes of the models, e.g. the class name, the modality type, etc.

The function returns a dataframe of the models that match the conditions. The dataframe contains the
method name, the class name, the modality type, the fusion type, the path to the model, and the path to the
model's parent class. The paths are used to import the models with the :func:`importlib.import_module`.

We're importing all the fusion models that use only tabular data for this example (either uni-modal or multi-modal).

.. GENERATED FROM PYTHON SOURCE LINES 71-78

.. code-block:: Python


    model_conditions = {
        "modality_type": ["tabular1", "tabular2", "tabular_tabular"],
    }

    fusion_models = import_chosen_fusion_models(model_conditions)



.. rst-class:: sphx-glr-script-out

.. code-block:: pytb

    Traceback (most recent call last):
      File "/Users/florencetownend/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Projects/fusilli/docs/examples/training_and_testing/plot_model_comparison_loop_kfold.py", line 76, in <module>
        fusion_models = import_chosen_fusion_models(model_conditions)
      File "/Users/florencetownend/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Projects/fusilli/fusilli/utils/model_chooser.py", line 323, in import_chosen_fusion_models
        imported_models = get_models(model_conditions, skip_models)
      File "/Users/florencetownend/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Projects/fusilli/fusilli/utils/model_chooser.py", line 194, in get_models
        fusion_models, fusion_model_dict_without_skips = all_model_importer(fusion_model_dict, skip_models=skip_models)
      File "/Users/florencetownend/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Projects/fusilli/fusilli/utils/model_chooser.py", line 125, in all_model_importer
        module = importlib.import_module(module_path)
      File "/Users/florencetownend/miniforge3/lib/python3.9/importlib/__init__.py", line 127, in import_module
        return _bootstrap._gcd_import(name[level:], package, level)
      File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
      File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
      File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
      File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
      File "<frozen importlib._bootstrap_external>", line 850, in exec_module
      File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
      File "/Users/florencetownend/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Projects/fusilli/fusilli/fusionmodels/tabularfusion/mcvae_model.py", line 9, in <module>
        from fusilli.utils.mcvae.src.mcvae.models import Mcvae
    ImportError: cannot import name 'Mcvae' from 'fusilli.utils.mcvae.src.mcvae.models' (unknown location)




.. GENERATED FROM PYTHON SOURCE LINES 79-92

2. Set the training parameters üéØ
---------------------------------
Let's configure our training parameters. The parameters are stored in a dictionary and passed to most
of the methods in this library.
For training and testing, the necessary parameters are:

- ``kfold_flag``: the user sets this to True for k-fold cross validation.
- ``num_k``: the number of folds to use. It can't be k=1.
- ``log``: a boolean of whether to log the results using Weights and Biases (True) or not (False).
- ``pred_type``: the type of prediction to be performed. This is either ``regression``, ``binary``, or ``classification``. For this example we're using regression.
- ``loss_log_dir``: the directory to save the loss logs to. This is used for plotting the loss curves with ``log=False``.

We're also setting our own batch_size for this example.

.. GENERATED FROM PYTHON SOURCE LINES 92-110

.. code-block:: Python



    params = {
        "kfold_flag": True,
        "num_k": 3,
        "log": False,
        "pred_type": "regression",
        "batch_size": 32,
        "loss_log_dir": "loss_logs/model_comparison_loop_kfold",
    }

    for dir in os.listdir(params["loss_log_dir"]):
        # remove files
        for file in os.listdir(os.path.join(params["loss_log_dir"], dir)):
            os.remove(os.path.join(params["loss_log_dir"], dir, file))
        # remove dir
        os.rmdir(os.path.join(params["loss_log_dir"], dir))


.. GENERATED FROM PYTHON SOURCE LINES 111-115

3. Generating simulated data üîÆ
--------------------------------
Time to create some simulated data for our models to work their wonders on.
This function also simulated image data which we aren't using here.

.. GENERATED FROM PYTHON SOURCE LINES 115-124

.. code-block:: Python


    params = generate_sklearn_simulated_data(
        num_samples=500,
        num_tab1_features=10,
        num_tab2_features=20,
        img_dims=(1, 100, 100),
        params=params,
    )


.. GENERATED FROM PYTHON SOURCE LINES 125-129

4. Training the all the fusion models üèÅ
-----------------------------------------
In this section, we train all the fusion models using the generated data and specified parameters.
We store the results of each model for later analysis.

.. GENERATED FROM PYTHON SOURCE LINES 129-155

.. code-block:: Python


    # Using %%capture to hide the progress bar and plots (there are a lot of them!)

    all_trained_models = {}

    for i, fusion_model in enumerate(fusion_models):
        fusion_model_name = fusion_model.__name__
        print(f"Running model {fusion_model_name}")

        # Get data module
        data_module = get_data_module(fusion_model, params, batch_size=params["batch_size"])

        # Train and test
        single_model_list = train_and_save_models(
            data_module=data_module,
            params=params,
            fusion_model=fusion_model,
            enable_checkpointing=False,  # False for the example notebooks
            show_loss_plot=True,  # True for the example notebooks
        )

        # Save to all_trained_models
        all_trained_models[fusion_model_name] = single_model_list

        plt.close("all")


.. GENERATED FROM PYTHON SOURCE LINES 156-163

5. Plotting the results of the individual models
-------------------------------------------------
In this section, we visualize the results of each individual model.

If you want to save the figures rather than show them, you can use the :meth:`~.save_to_local' method of the :class:`~fusilli.eval.Plotter` class.
This will save the figures in a timestamped folder in the current working directory with the method name and plot type in the filename.
You can add an extra suffix to the filename by passing a string to the ``extra_string`` argument of the :meth:`~fusilli.eval.Plotter.save_to_local` method.

.. GENERATED FROM PYTHON SOURCE LINES 163-168

.. code-block:: Python


    for model_name, model_list in all_trained_models.items():
        fig = RealsVsPreds.from_final_val_data(model_list)
        plt.show()


.. GENERATED FROM PYTHON SOURCE LINES 169-172

6. Plotting comparison of the models
-------------------------------------
In this section, we visualize the results of each individual model.

.. GENERATED FROM PYTHON SOURCE LINES 172-176

.. code-block:: Python


    comparison_plot, metrics_dataframe = ModelComparison.from_final_val_data(all_trained_models)
    plt.show()


.. GENERATED FROM PYTHON SOURCE LINES 177-180

7. Saving the results of the models
-------------------------------------
In this section, we compare the performance of all the trained models using a violin chart, providing an overview of how each model performed as a distribution over the different cross-validation folds.

.. GENERATED FROM PYTHON SOURCE LINES 180-183

.. code-block:: Python



    metrics_dataframe


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 0.080 seconds)


.. _sphx_glr_download_auto_examples_training_and_testing_plot_model_comparison_loop_kfold.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_model_comparison_loop_kfold.ipynb <plot_model_comparison_loop_kfold.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_model_comparison_loop_kfold.py <plot_model_comparison_loop_kfold.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
